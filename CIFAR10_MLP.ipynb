{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10-MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reallyclean-sauce/WazzUP/blob/master/CIFAR10_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXgmlqo5QglQ",
        "colab_type": "text"
      },
      "source": [
        "### CIFAR-10 Using Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsYZP60MQzOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# numpy package\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.image import rgb_to_grayscale\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Regularizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A6k1OHnQxJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import cifar10 dataset\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# Load cifar10 Dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJvWyR4WC-mW",
        "colab_type": "code",
        "outputId": "b49e02f0-178e-45bf-d953-10a085fe4df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(np.unique(y_train))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp2LwGydSd7A",
        "colab_type": "text"
      },
      "source": [
        "CIFAR-10\n",
        "- Contains 3 channels\n",
        "- 10 Classes\n",
        "\n",
        "Convert to\n",
        "- Single-channel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBymBnWZTi4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "# x_new = rgb_to_grayscale(x_train)\n",
        "\n",
        "x_train_gray = np.dot(x_train[:,:,:,:3], [0.299, 0.587, 0.114])\n",
        "x_test_gray = np.dot(x_test[:,:,:,:3], [0.299, 0.587, 0.114])\n",
        "\n",
        "# x_train_gray = x_train_gray.reshape(-1,32,32,1)\n",
        "# x_test_gray = x_test_gray.reshape(-1,32,32,1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMLupgiIWWTe",
        "colab_type": "code",
        "outputId": "b976b6e2-973c-44ec-b371-445b2010b3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fU85KaRXEuq",
        "colab_type": "code",
        "outputId": "10082d68-edf0-40fe-85f2-931dc6d9e073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_gray.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNJogjQqTAmg",
        "colab_type": "code",
        "outputId": "957d21a1-e2b3-4e6c-c9f3-3f17ba732464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "plt.imshow(x_train[1])\n",
        "plt.show()\n",
        "\n",
        "# plt.imshow(x_train_gray[1,:,:,0], cmap='gray')\n",
        "# plt.show()\n",
        "\n",
        "plt.imshow(x_train_gray[1])\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKAp+IRKKgl+IhKLgFyKhKPiFSCj/D/EcoRMKOMsFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36vIeIZNSsmf",
        "colab_type": "code",
        "outputId": "4437aeb5-28e4-4590-df69-c081fb2ebfe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# compute the number of labels\n",
        "num_labels = len(np.unique(y_train))\n",
        "num_labels"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfUN51_9DTGm",
        "colab_type": "code",
        "outputId": "89a6fc45-bc01-4747-b692-0c7240fbd64c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# convert to one-hot vector\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_labels"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtg8jxphDUol",
        "colab_type": "code",
        "outputId": "c32b1519-6efb-49e8-ff6a-cac1ea2083cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# image dimensions (assumed square)\n",
        "image_size = x_train_gray.shape[1]\n",
        "input_size = image_size * image_size\n",
        "# for mlp, the input dim is a vector, so we reshape\n",
        "print(x_train_gray.shape)\n",
        "x_train_gray = np.reshape(x_train_gray, [-1, input_size])\n",
        "print(x_train_gray.shape)\n",
        "# we train our network using float data\n",
        "x_train_gray = x_train_gray.astype('float32') / 255\n",
        "x_test_gray = np.reshape(x_test_gray, [-1, input_size])\n",
        "x_test_gray = x_test_gray.astype('float32') / 255\n",
        "num_labels"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32)\n",
            "(50000, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4XWsdxcJBzO",
        "colab_type": "code",
        "outputId": "0bb685f2-944e-4c55-99a9-ab1d48af6048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfWXWutzCu5f",
        "colab_type": "code",
        "outputId": "bc26cb34-b71c-4b09-dfee-1a3261f96f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_labels"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGS-kl1AS-SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# network parameters\n",
        "batch_size = 128\n",
        "hidden_units = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45lN6vDfZgmI",
        "colab_type": "text"
      },
      "source": [
        "#### First Iteration\n",
        "Model:\n",
        "- 3 layers\n",
        "- MLP\n",
        "- Weights Initializer: (Default) Glorot_uniform\n",
        "- Activation: ReLU\n",
        "- Final Activation: softmax\n",
        "- Regularization: None\n",
        "- Optimizer: SGD\n",
        "- Loss function: cross-entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpLE8RX_UbPr",
        "colab_type": "code",
        "outputId": "43dbb5fe-cd4a-4597-c80c-3a4a96197963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "# this is 3-layer MLP with ReLU. No regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT7oB9wlUdfL",
        "colab_type": "code",
        "outputId": "edc783bb-e158-432e-b9cc-9646f9e945f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "source": [
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 3s 60us/sample - loss: 2.1600 - acc: 0.2117\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 55us/sample - loss: 2.0576 - acc: 0.2669\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 3s 54us/sample - loss: 2.0132 - acc: 0.2852\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 3s 55us/sample - loss: 1.9831 - acc: 0.3003\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 3s 53us/sample - loss: 1.9576 - acc: 0.3113\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 3s 53us/sample - loss: 1.9334 - acc: 0.3213\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 3s 51us/sample - loss: 1.9100 - acc: 0.3309\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 3s 53us/sample - loss: 1.8883 - acc: 0.3384\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 3s 51us/sample - loss: 1.8689 - acc: 0.3462\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 3s 53us/sample - loss: 1.8506 - acc: 0.3520\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 3s 55us/sample - loss: 1.8345 - acc: 0.3574\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 3s 55us/sample - loss: 1.8197 - acc: 0.3608\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.8058 - acc: 0.3661\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.7924 - acc: 0.3702\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.7799 - acc: 0.3754\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 1.7682 - acc: 0.3827\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.7565 - acc: 0.3852\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 3s 59us/sample - loss: 1.7442 - acc: 0.3885\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 3s 59us/sample - loss: 1.7344 - acc: 0.3920\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 3s 55us/sample - loss: 1.7247 - acc: 0.3924\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 1.7572 - acc: 0.3831\n",
            "\n",
            "Test accuracy: 38.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzaPOpxwZPpC",
        "colab_type": "text"
      },
      "source": [
        "#### Second Iteration\n",
        "Model:\n",
        "- 3 layers\n",
        "- MLP\n",
        "- Activation: ReLU\n",
        "- Final Activation: softmax\n",
        "- Regularization: L2(0.1) -> L2(0.01) -> L2(0.001) -> L2(0.0001)\n",
        "- Optimizer: SGD\n",
        "- Loss function: cross-entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZOyMfpBdbrn",
        "colab_type": "code",
        "outputId": "23f41cb4-d67e-4345-a654-ea74e60f35b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.1)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 3s 64us/sample - loss: 36.0297 - acc: 0.2010\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 9.3184 - acc: 0.2167\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 3.7409 - acc: 0.2051\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 3s 59us/sample - loss: 2.5691 - acc: 0.2034\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 3s 59us/sample - loss: 2.3182 - acc: 0.2018\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 2.2609 - acc: 0.2019\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 2.2450 - acc: 0.2061\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 2.2379 - acc: 0.2057\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 2.2331 - acc: 0.2082\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 2.2291 - acc: 0.2102\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 2.2257 - acc: 0.2132\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 2.2224 - acc: 0.2131\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 2.2195 - acc: 0.2141\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 2.2167 - acc: 0.2186\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 2.2141 - acc: 0.2195\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.2117 - acc: 0.2200\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 2.2095 - acc: 0.2212\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 2.2070 - acc: 0.2231\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 2.2053 - acc: 0.2241\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 2.2035 - acc: 0.2251\n",
            "10000/10000 [==============================] - 0s 32us/sample - loss: 2.2027 - acc: 0.2239\n",
            "\n",
            "Test accuracy: 22.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1oRIuBmZTCq",
        "colab_type": "code",
        "outputId": "f57b788c-a331-4cd6-a232-e57bcc8f0029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.01)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 8.3205 - acc: 0.2194\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 7.3455 - acc: 0.2640\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 6.5580 - acc: 0.2824\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 5.8934 - acc: 0.2924\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 4s 74us/sample - loss: 5.3281 - acc: 0.2998\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 4.8447 - acc: 0.3079\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 4.4311 - acc: 0.3121\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 4.0778 - acc: 0.3162\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 4s 74us/sample - loss: 3.7745 - acc: 0.3206\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 3.5147 - acc: 0.3225\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 4s 74us/sample - loss: 3.2919 - acc: 0.3274\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 3.1011 - acc: 0.3314\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 70us/sample - loss: 2.9370 - acc: 0.3331\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.7950 - acc: 0.3382\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 2.6738 - acc: 0.3390\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.5706 - acc: 0.3416\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 2.4805 - acc: 0.3443\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 2.4030 - acc: 0.3455\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 2.3382 - acc: 0.3462\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 2.2801 - acc: 0.3495\n",
            "10000/10000 [==============================] - 0s 35us/sample - loss: 2.2632 - acc: 0.3530\n",
            "\n",
            "Test accuracy: 35.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyxQO4w9Yjmp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTGXL9RfUexD",
        "colab_type": "code",
        "outputId": "f6c17da5-86fd-46d4-ae44-9cc275cf05e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 2.8152 - acc: 0.2131\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 2.7031 - acc: 0.2655\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 2.6536 - acc: 0.2835\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 4s 74us/sample - loss: 2.6185 - acc: 0.2962\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 2.5848 - acc: 0.3074\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 3s 70us/sample - loss: 2.5550 - acc: 0.3176\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 2.5256 - acc: 0.3259\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 2.4966 - acc: 0.3337\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 2.4699 - acc: 0.3416\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 2.4451 - acc: 0.3493\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 2.4204 - acc: 0.3544\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 2.3984 - acc: 0.3584\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 2.3780 - acc: 0.3646\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 2.3582 - acc: 0.3680\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 2.3389 - acc: 0.3715\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 3s 64us/sample - loss: 2.3216 - acc: 0.3755\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 3s 65us/sample - loss: 2.3040 - acc: 0.3789\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 2.2872 - acc: 0.3820\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 2.2699 - acc: 0.3867\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 2.2550 - acc: 0.3916\n",
            "10000/10000 [==============================] - 0s 31us/sample - loss: 2.2629 - acc: 0.3819\n",
            "\n",
            "Test accuracy: 38.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9eQziS2dSS1",
        "colab_type": "code",
        "outputId": "7e7ef9fa-0d70-4659-af54-a6436a21d992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 3s 60us/sample - loss: 2.2141 - acc: 0.2217\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 2.1117 - acc: 0.2690\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 2.0709 - acc: 0.2897\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 4s 70us/sample - loss: 2.0406 - acc: 0.3060\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 2.0138 - acc: 0.3158\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.9890 - acc: 0.3262\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 3s 57us/sample - loss: 1.9666 - acc: 0.3330\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 3s 60us/sample - loss: 1.9459 - acc: 0.3413\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 3s 60us/sample - loss: 1.9259 - acc: 0.3484\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 3s 64us/sample - loss: 1.9092 - acc: 0.3535\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 3s 59us/sample - loss: 1.8932 - acc: 0.3612\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 1.8785 - acc: 0.3644\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.8645 - acc: 0.3702\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.8534 - acc: 0.3729\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.8407 - acc: 0.3796\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 3s 57us/sample - loss: 1.8297 - acc: 0.3830\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 3s 59us/sample - loss: 1.8207 - acc: 0.3861\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.8080 - acc: 0.3897\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.7987 - acc: 0.3941\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 3s 57us/sample - loss: 1.7896 - acc: 0.3967\n",
            "10000/10000 [==============================] - 0s 29us/sample - loss: 1.8249 - acc: 0.3766\n",
            "\n",
            "Test accuracy: 37.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jH9KbufoToy",
        "colab_type": "text"
      },
      "source": [
        "@ L2(0.0001): peak test and training accuracy for training and test data\n",
        "\n",
        "#### Explanation\n",
        "Unknown as of the moment!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihwfy0AOkytM",
        "colab_type": "code",
        "outputId": "bd507fd3-022d-40db-dd32-268fd22d815b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.00001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 2.1600 - acc: 0.2144\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 57us/sample - loss: 2.0620 - acc: 0.2636\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 2.0184 - acc: 0.2843\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 3s 60us/sample - loss: 1.9878 - acc: 0.3001\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 3s 57us/sample - loss: 1.9608 - acc: 0.3127\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 3s 57us/sample - loss: 1.9360 - acc: 0.3212\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.9136 - acc: 0.3300\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.8923 - acc: 0.3377\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.8730 - acc: 0.3438\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.8558 - acc: 0.3517\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.8394 - acc: 0.3547\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.8249 - acc: 0.3631\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.8108 - acc: 0.3670\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 3s 60us/sample - loss: 1.7987 - acc: 0.3730\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 3s 57us/sample - loss: 1.7858 - acc: 0.3763\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 3s 64us/sample - loss: 1.7754 - acc: 0.3806\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 1.7642 - acc: 0.3849\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 70us/sample - loss: 1.7538 - acc: 0.3883\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 1.7432 - acc: 0.3908\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 3s 59us/sample - loss: 1.7324 - acc: 0.3943\n",
            "10000/10000 [==============================] - 0s 30us/sample - loss: 1.7677 - acc: 0.3731\n",
            "\n",
            "Test accuracy: 37.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDIi0N5vk1ZM",
        "colab_type": "code",
        "outputId": "ac839e38-6350-4da7-dee9-d397c720ebad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.000001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 2.1560 - acc: 0.2202\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 2.0512 - acc: 0.2700\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 2.0066 - acc: 0.2899\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 3s 59us/sample - loss: 1.9759 - acc: 0.3022\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 3s 58us/sample - loss: 1.9496 - acc: 0.3142\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.9261 - acc: 0.3233\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 1.9046 - acc: 0.3296\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.8849 - acc: 0.3389\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 1.8675 - acc: 0.3432\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 3s 59us/sample - loss: 1.8510 - acc: 0.3478\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 3s 60us/sample - loss: 1.8365 - acc: 0.3543\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 1.8221 - acc: 0.3593\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 1.8088 - acc: 0.3636\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 1.7966 - acc: 0.3692\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.7853 - acc: 0.3725\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 1.7740 - acc: 0.3776\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 1.7627 - acc: 0.3803\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 3s 65us/sample - loss: 1.7524 - acc: 0.3854\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 3s 65us/sample - loss: 1.7425 - acc: 0.3869\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 1.7336 - acc: 0.3919\n",
            "10000/10000 [==============================] - 0s 37us/sample - loss: 1.7579 - acc: 0.3790\n",
            "\n",
            "Test accuracy: 37.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXfYSsE0np75",
        "colab_type": "code",
        "outputId": "638e6c70-626a-4475-f95c-bd50495dd432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.000000001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_21 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 2.1605 - acc: 0.2183\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 2.0570 - acc: 0.2704\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 3s 60us/sample - loss: 2.0142 - acc: 0.2889\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 1.9844 - acc: 0.3016\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 3s 60us/sample - loss: 1.9588 - acc: 0.3127\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.9348 - acc: 0.3221\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 3s 64us/sample - loss: 1.9119 - acc: 0.3312\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 1.8912 - acc: 0.3377\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 3s 65us/sample - loss: 1.8704 - acc: 0.3440\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 3s 65us/sample - loss: 1.8527 - acc: 0.3504\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 1.8362 - acc: 0.3560\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 3s 65us/sample - loss: 1.8216 - acc: 0.3609\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 1.8076 - acc: 0.3668\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 1.7960 - acc: 0.3709\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 3s 65us/sample - loss: 1.7830 - acc: 0.3727\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 1.7721 - acc: 0.3781\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 3s 59us/sample - loss: 1.7609 - acc: 0.3825\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 1.7503 - acc: 0.3854\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 1.7395 - acc: 0.3901\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.7297 - acc: 0.3940\n",
            "10000/10000 [==============================] - 0s 33us/sample - loss: 1.7536 - acc: 0.3803\n",
            "\n",
            "Test accuracy: 38.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvzRVgI-dfIM",
        "colab_type": "text"
      },
      "source": [
        "#### Third Iteration\n",
        "Model:\n",
        "- 3 layers\n",
        "- MLP\n",
        "- Activation: ReLU\n",
        "- Final Activation: softmax\n",
        "- Regularization: Dropout = 0.2 -> 0.4 -> 0.5 -> 0.6 -> 0.8\n",
        "- Optimizer: SGD\n",
        "- Loss function: cross-entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nhMwyGQtGF5",
        "colab_type": "code",
        "outputId": "4712d49f-9f71-458c-b2e7-825ac7449c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Dropout Regularizer\n",
        "dropout = 0.1\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 2.1881 - acc: 0.1906\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 64us/sample - loss: 2.0811 - acc: 0.2475\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 3s 62us/sample - loss: 2.0360 - acc: 0.2679\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 2.0029 - acc: 0.2861\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 1.9779 - acc: 0.2975\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 1.9527 - acc: 0.3085\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 1.9320 - acc: 0.3203\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 3s 63us/sample - loss: 1.9126 - acc: 0.3256\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 3s 61us/sample - loss: 1.8937 - acc: 0.3299\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 3s 64us/sample - loss: 1.8795 - acc: 0.3367\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 3s 64us/sample - loss: 1.8671 - acc: 0.3423\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 1.8529 - acc: 0.3474\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 1.8403 - acc: 0.3539\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 1.8293 - acc: 0.3547\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 1.8191 - acc: 0.3572\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 3s 70us/sample - loss: 1.8059 - acc: 0.3656\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 1.7987 - acc: 0.3678\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 1.7882 - acc: 0.3721\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 1.7783 - acc: 0.3748\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 4s 70us/sample - loss: 1.7703 - acc: 0.3772\n",
            "10000/10000 [==============================] - 0s 31us/sample - loss: 1.7592 - acc: 0.3767\n",
            "\n",
            "Test accuracy: 37.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gmlUdTPdeWO",
        "colab_type": "code",
        "outputId": "62f69a00-59a0-4e8b-cc4d-6a8b7a941c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Dropout Regularizer\n",
        "dropout = 0.2\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_27 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 2.2078 - acc: 0.1800\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 2.1124 - acc: 0.2301\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 2.0631 - acc: 0.2558\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 2.0322 - acc: 0.2706\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 2.0075 - acc: 0.2855\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 4s 82us/sample - loss: 1.9844 - acc: 0.2915\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 1.9654 - acc: 0.3029\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 4s 79us/sample - loss: 1.9463 - acc: 0.3093\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 1.9289 - acc: 0.3183\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 4s 81us/sample - loss: 1.9138 - acc: 0.3203\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 4s 81us/sample - loss: 1.8994 - acc: 0.3276\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 1.8853 - acc: 0.3335\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 82us/sample - loss: 1.8716 - acc: 0.3367\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 1.8605 - acc: 0.3424\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 1.8502 - acc: 0.3458\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 1.8394 - acc: 0.3497\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 1.8298 - acc: 0.3551\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 1.8251 - acc: 0.3539\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 4s 74us/sample - loss: 1.8130 - acc: 0.3583\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 1.8042 - acc: 0.3614\n",
            "10000/10000 [==============================] - 0s 41us/sample - loss: 1.7696 - acc: 0.3853\n",
            "\n",
            "Test accuracy: 38.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhmhEyGIeXmy",
        "colab_type": "code",
        "outputId": "f2753fc2-4f5f-4392-8eb2-b12f8f2e0ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Dropout Regularizer\n",
        "dropout = 0.4\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_30 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 4s 74us/sample - loss: 2.2517 - acc: 0.1617\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 2.1615 - acc: 0.2033\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 2.1113 - acc: 0.2294\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 2.0816 - acc: 0.2429\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 4s 70us/sample - loss: 2.0591 - acc: 0.2546\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 2.0408 - acc: 0.2646\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 2.0226 - acc: 0.2745\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 2.0084 - acc: 0.2776\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 3s 70us/sample - loss: 1.9928 - acc: 0.2852\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 1.9814 - acc: 0.2935\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 1.9684 - acc: 0.2971\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 1.9584 - acc: 0.2974\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 1.9489 - acc: 0.3027\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 1.9341 - acc: 0.3114\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 1.9296 - acc: 0.3107\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 1.9185 - acc: 0.3164\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 1.9114 - acc: 0.3187\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 1.9037 - acc: 0.3237\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 1.8963 - acc: 0.3275\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 1.8870 - acc: 0.3308\n",
            "10000/10000 [==============================] - 0s 40us/sample - loss: 1.8229 - acc: 0.3623\n",
            "\n",
            "Test accuracy: 36.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ3R9kgCeZVZ",
        "colab_type": "code",
        "outputId": "9bf83f69-ba38-4929-afac-1466bf6752ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Dropout Regularizer\n",
        "dropout = 0.5\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_33 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.2813 - acc: 0.1449\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 2.1959 - acc: 0.1848\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.1544 - acc: 0.2041\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.1225 - acc: 0.2229\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 2.0975 - acc: 0.2372\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 2.0782 - acc: 0.2443\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.0646 - acc: 0.2521\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.0476 - acc: 0.2597\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 2.0358 - acc: 0.2662\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 2.0266 - acc: 0.2729\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 2.0084 - acc: 0.2750\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 2.0039 - acc: 0.2790\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 1.9924 - acc: 0.2848\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 1.9821 - acc: 0.2901\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 1.9703 - acc: 0.2955\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 1.9657 - acc: 0.2961\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 1.9572 - acc: 0.2976\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 1.9513 - acc: 0.3009\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 4s 82us/sample - loss: 1.9390 - acc: 0.3068\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 1.9307 - acc: 0.3087\n",
            "10000/10000 [==============================] - 0s 45us/sample - loss: 1.8595 - acc: 0.3502\n",
            "\n",
            "Test accuracy: 35.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs0O_Kxvea08",
        "colab_type": "code",
        "outputId": "e4f5b694-239b-4612-c226-4c915133b75d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Dropout Regularizer\n",
        "dropout = 0.6\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_36 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 2.2981 - acc: 0.1342\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 4s 74us/sample - loss: 2.2284 - acc: 0.1666\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 2.1955 - acc: 0.1866\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 4s 70us/sample - loss: 2.1649 - acc: 0.2022\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 4s 70us/sample - loss: 2.1437 - acc: 0.2097\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 2.1226 - acc: 0.2208\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 3s 70us/sample - loss: 2.1070 - acc: 0.2291\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 2.0915 - acc: 0.2391\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 2.0802 - acc: 0.2447\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 2.0722 - acc: 0.2484\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 2.0589 - acc: 0.2526\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 4s 74us/sample - loss: 2.0508 - acc: 0.2610\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 2.0402 - acc: 0.2631\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 2.0362 - acc: 0.2670\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 2.0223 - acc: 0.2706\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 2.0170 - acc: 0.2748\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 2.0049 - acc: 0.2797\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 79us/sample - loss: 2.0045 - acc: 0.2772\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 1.9949 - acc: 0.2831\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 1.9873 - acc: 0.2855\n",
            "10000/10000 [==============================] - 0s 43us/sample - loss: 1.9021 - acc: 0.3398\n",
            "\n",
            "Test accuracy: 34.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTGIaHFvecF7",
        "colab_type": "code",
        "outputId": "401c3015-5959-45bb-b18c-d177a3081ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Dropout Regularizer\n",
        "dropout = 0.8\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_39 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 4s 79us/sample - loss: 2.3338 - acc: 0.1080\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 2.2943 - acc: 0.1233\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 2.2851 - acc: 0.1368\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.2719 - acc: 0.1496\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 4s 79us/sample - loss: 2.2616 - acc: 0.1529\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.2512 - acc: 0.1550\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 4s 79us/sample - loss: 2.2412 - acc: 0.1624\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 2.2316 - acc: 0.1644\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 2.2234 - acc: 0.1653\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 2.2164 - acc: 0.1709\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 2.2081 - acc: 0.1734\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 2.2021 - acc: 0.1738\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 2.1984 - acc: 0.1789\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 2.1929 - acc: 0.1770\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 2.1884 - acc: 0.1792\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 2.1834 - acc: 0.1825\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 2.1762 - acc: 0.1868\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 74us/sample - loss: 2.1735 - acc: 0.1868\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 3s 68us/sample - loss: 2.1684 - acc: 0.1896\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 2.1686 - acc: 0.1907\n",
            "10000/10000 [==============================] - 0s 41us/sample - loss: 2.1361 - acc: 0.2346\n",
            "\n",
            "Test accuracy: 23.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeghlVn-ekFZ",
        "colab_type": "text"
      },
      "source": [
        "#### Fourth Iteration\n",
        "Model:\n",
        "- 3 layers\n",
        "- MLP\n",
        "- Activation: ReLU\n",
        "- Final Activation: softmax\n",
        "- Regularization: Data Augmentation\n",
        "  - Horizontal Flip\n",
        "  - Vertical Flip\n",
        "  - Rotation range\n",
        "  - Width shift range\n",
        "  - Height shift range\n",
        "- Optimizer: SGD\n",
        "- Loss function: cross-entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRgUUTZ1e3IM",
        "colab_type": "code",
        "outputId": "55d695ce-d957-4f09-c19e-5af1299087c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "data_augmentation = True\n",
        "epochs = 20\n",
        "max_batches = len(x_train) / batch_size\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    # train the network no data augmentation\n",
        "    x_train_gray = np.reshape(x_train_gray, [-1, input_size])\n",
        "    model.fit(x_train_gray, y_train, epochs=epochs, batch_size=batch_size)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    # we need [width, height, channel] dim for data aug\n",
        "    x_train_gray = np.reshape(x_train_gray, [-1, image_size, image_size, 1])\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=5.0,  # randomly rotate images in the range (deg 0 to 180)\n",
        "        width_shift_range=2.0,  # randomly shift images horizontally\n",
        "        height_shift_range=2.0,  # randomly shift images vertically\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True)  # randomly flip images\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train_gray)\n",
        "    for e in range(epochs):\n",
        "        batches = 0\n",
        "        for x_batch, y_batch in datagen.flow(x_train_gray, y_train, batch_size=batch_size):\n",
        "            x_batch = np.reshape(x_batch, [-1, input_size])\n",
        "            model.fit(x_batch, y_batch, verbose=0)\n",
        "            batches += 1\n",
        "            \n",
        "            if batches >= max_batches:\n",
        "                # we need to break the loop by hand because\n",
        "                # the generator loops indefinitely\n",
        "                break\n",
        "        print(\"Epoch %d/%d\" % (e+1, epochs))\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_63 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/20\n",
            "Epoch 2/20\n",
            "Epoch 3/20\n",
            "Epoch 4/20\n",
            "Epoch 5/20\n",
            "Epoch 6/20\n",
            "Epoch 7/20\n",
            "Epoch 8/20\n",
            "Epoch 9/20\n",
            "Epoch 10/20\n",
            "Epoch 11/20\n",
            "Epoch 12/20\n",
            "Epoch 13/20\n",
            "Epoch 14/20\n",
            "Epoch 15/20\n",
            "Epoch 16/20\n",
            "Epoch 17/20\n",
            "Epoch 18/20\n",
            "Epoch 19/20\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 0s 44us/sample - loss: 1.7717 - acc: 0.3713\n",
            "\n",
            "Test accuracy: 37.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZd8rHDMjVjE",
        "colab_type": "text"
      },
      "source": [
        "#### Fifth Iteration\n",
        "Model:\n",
        "- 3 layers\n",
        "- MLP\n",
        "- Activation: ReLU\n",
        "- Final Activation: softmax\n",
        "- Regularization: \n",
        "  - Data Augmentation\n",
        "    - Horizontal Flip\n",
        "    - Vertical Flip\n",
        "    - Rotation range\n",
        "    - Width shift range\n",
        "    - Height shift range\n",
        "  - Dropout: 0.2\n",
        "  - L2: 0.0001\n",
        "- Optimizer: SGD\n",
        "- Loss function: cross-entropy\n",
        "\n",
        "Combined the best of each regularizer algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18mm1WStmeno",
        "colab_type": "code",
        "outputId": "2b33eb57-e15d-4431-b821-042b7592752f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "dropout = 0.2\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "data_augmentation = True\n",
        "epochs = 20\n",
        "max_batches = len(x_train) / batch_size\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    # train the network no data augmentation\n",
        "    x_train_gray = np.reshape(x_train_gray, [-1, input_size])\n",
        "    model.fit(x_train_gray, y_train, epochs=epochs, batch_size=batch_size)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    # we need [width, height, channel] dim for data aug\n",
        "    x_train_gray = np.reshape(x_train_gray, [-1, image_size, image_size, 1])\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=5.0,  # randomly rotate images in the range (deg 0 to 180)\n",
        "        width_shift_range=2.0,  # randomly shift images horizontally\n",
        "        height_shift_range=2.0,  # randomly shift images vertically\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True)  # randomly flip images\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train_gray)\n",
        "    for e in range(epochs):\n",
        "        batches = 0\n",
        "        for x_batch, y_batch in datagen.flow(x_train_gray, y_train, batch_size=batch_size):\n",
        "            x_batch = np.reshape(x_batch, [-1, input_size])\n",
        "            model.fit(x_batch, y_batch, verbose=0)\n",
        "            batches += 1\n",
        "            \n",
        "            if batches >= max_batches:\n",
        "                # we need to break the loop by hand because\n",
        "                # the generator loops indefinitely\n",
        "                break\n",
        "        print(\"Epoch %d/%d\" % (e+1, epochs))\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_69 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_69 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_70 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_71 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/20\n",
            "Epoch 2/20\n",
            "Epoch 3/20\n",
            "Epoch 4/20\n",
            "Epoch 5/20\n",
            "Epoch 6/20\n",
            "Epoch 7/20\n",
            "Epoch 8/20\n",
            "Epoch 9/20\n",
            "Epoch 10/20\n",
            "Epoch 11/20\n",
            "Epoch 12/20\n",
            "Epoch 13/20\n",
            "Epoch 14/20\n",
            "Epoch 15/20\n",
            "Epoch 16/20\n",
            "Epoch 17/20\n",
            "Epoch 18/20\n",
            "Epoch 19/20\n",
            "Epoch 20/20\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 1.8448 - acc: 0.3731\n",
            "\n",
            "Test accuracy: 37.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WqO8nHnoO25",
        "colab_type": "text"
      },
      "source": [
        "It yielded a result less than the individual's algorithm's best accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLnbSaglqTNJ",
        "colab_type": "text"
      },
      "source": [
        "#### Summary\n",
        "It is better to use a single regularizer.\n",
        "\n",
        "Hypothesis: Regularizer penalizes the algorithm so that it would not overfit. By combining two regularizers, we penalize the algorithm in such a way that it is too much. Thus, the result is an underfitted algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93HoBziOoP5_",
        "colab_type": "text"
      },
      "source": [
        "### Changing the SGD algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUbbyCoVoloi",
        "colab_type": "code",
        "outputId": "5de5d78a-93df-4811-b081-bfda6991924f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "# Load cifar10 Dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train_gray = np.dot(x_train[:,:,:,:3], [0.299, 0.587, 0.114])\n",
        "x_test_gray = np.dot(x_test[:,:,:,:3], [0.299, 0.587, 0.114])\n",
        "\n",
        "# convert to one-hot vector\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# image dimensions (assumed square)\n",
        "image_size = x_train_gray.shape[1]\n",
        "input_size = image_size * image_size\n",
        "\n",
        "# for mlp, the input dim is a vector, so we reshape\n",
        "print(x_train_gray.shape)\n",
        "x_train_gray = np.reshape(x_train_gray, [-1, input_size])\n",
        "print(x_train_gray.shape)\n",
        "# we train our network using float data\n",
        "x_train_gray = x_train_gray.astype('float32') / 255\n",
        "x_test_gray = np.reshape(x_test_gray, [-1, input_size])\n",
        "x_test_gray = x_test_gray.astype('float32') / 255\n",
        "\n",
        "num_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32)\n",
            "(50000, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB4MpxI4oW1V",
        "colab_type": "text"
      },
      "source": [
        "#### First Iteration\n",
        "Model:\n",
        "- 3 layers\n",
        "- MLP\n",
        "- Activation: ReLU\n",
        "- Final Activation: softmax\n",
        "- Regularization: L2: 0.0001\n",
        "- Optimizer: adagrad\n",
        "- Loss function: cross-entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1niInwTZnRna",
        "colab_type": "code",
        "outputId": "f50e13af-aa78-40a5-d1a4-2d01705923d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adagrad',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_72 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_72 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_73 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_74 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 2.1319 - acc: 0.2571\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 4s 84us/sample - loss: 2.0363 - acc: 0.2990\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 1.9982 - acc: 0.3171\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 4s 81us/sample - loss: 1.9712 - acc: 0.3262\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 1.9501 - acc: 0.3347\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 1.9327 - acc: 0.3407\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 1.9174 - acc: 0.3460\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 1.9042 - acc: 0.3526\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 4s 81us/sample - loss: 1.8932 - acc: 0.3561\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 1.8833 - acc: 0.3593\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 1.8739 - acc: 0.3635\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 1.8658 - acc: 0.3657\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 86us/sample - loss: 1.8580 - acc: 0.3699\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 1.8512 - acc: 0.3714\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 4s 70us/sample - loss: 1.8446 - acc: 0.3732\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 4s 81us/sample - loss: 1.8382 - acc: 0.3758\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 4s 87us/sample - loss: 1.8326 - acc: 0.3779\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 82us/sample - loss: 1.8270 - acc: 0.3795\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 1.8217 - acc: 0.3815\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 4s 87us/sample - loss: 1.8167 - acc: 0.3848\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 1.8359 - acc: 0.3752\n",
            "\n",
            "Test accuracy: 37.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpHGoZN8ofiZ",
        "colab_type": "text"
      },
      "source": [
        "#### Second Iteration\n",
        "Model:\n",
        "- 3 layers\n",
        "- MLP\n",
        "- Activation: ReLU\n",
        "- Final Activation: softmax\n",
        "- Regularization: L2: 0.0001\n",
        "- Optimizer: rmsprop\n",
        "- Loss function: cross-entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Foye40Tmojjb",
        "colab_type": "code",
        "outputId": "78aae2ad-56dc-4ee0-ce2b-482b578e3e58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_75 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_75 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_76 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_77 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 4s 89us/sample - loss: 2.1196 - acc: 0.2400\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 4s 82us/sample - loss: 1.9361 - acc: 0.3135\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 4s 83us/sample - loss: 1.8673 - acc: 0.3435\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 4s 84us/sample - loss: 1.8141 - acc: 0.3620\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 1.7707 - acc: 0.3796\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 4s 79us/sample - loss: 1.7406 - acc: 0.3914\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 4s 81us/sample - loss: 1.7146 - acc: 0.4034\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 4s 81us/sample - loss: 1.6871 - acc: 0.4100\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 4s 77us/sample - loss: 1.6656 - acc: 0.4183\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 4s 83us/sample - loss: 1.6489 - acc: 0.4282\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 4s 90us/sample - loss: 1.6281 - acc: 0.4341\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 4s 83us/sample - loss: 1.6143 - acc: 0.4385\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 82us/sample - loss: 1.6000 - acc: 0.4447\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 4s 84us/sample - loss: 1.5851 - acc: 0.4517\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 4s 79us/sample - loss: 1.5738 - acc: 0.4569\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 1.5591 - acc: 0.4637\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 4s 81us/sample - loss: 1.5492 - acc: 0.4672\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 79us/sample - loss: 1.5376 - acc: 0.4728\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 4s 82us/sample - loss: 1.5264 - acc: 0.4761\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 4s 84us/sample - loss: 1.5158 - acc: 0.4813\n",
            "10000/10000 [==============================] - 0s 41us/sample - loss: 1.6906 - acc: 0.4266\n",
            "\n",
            "Test accuracy: 42.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaSSfzByoj6c",
        "colab_type": "text"
      },
      "source": [
        "#### Third Iteration\n",
        "Model:\n",
        "- 3 layers\n",
        "- MLP\n",
        "- Activation: ReLU\n",
        "- Final Activation: softmax\n",
        "- Regularization: L2: 0.0001\n",
        "- Optimizer: adam\n",
        "- Loss function: cross-entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5ViIoMDprKm",
        "colab_type": "code",
        "outputId": "8e54e543-a8c4-4007-b3de-fc97396fcdee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_78 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_78 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_79 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_80 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 2.0673 - acc: 0.2660\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 1.9039 - acc: 0.3312\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 1.8363 - acc: 0.3561\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 3s 69us/sample - loss: 1.7912 - acc: 0.3720\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 3s 66us/sample - loss: 1.7546 - acc: 0.3858\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 4s 71us/sample - loss: 1.7219 - acc: 0.3985\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 1.6929 - acc: 0.4094\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 1.6688 - acc: 0.4184\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 1.6492 - acc: 0.4251\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 4s 78us/sample - loss: 1.6287 - acc: 0.4336\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 4s 75us/sample - loss: 1.6111 - acc: 0.4415\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 1.5945 - acc: 0.4477\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 4s 80us/sample - loss: 1.5775 - acc: 0.4536\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 1.5646 - acc: 0.4571\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 4s 72us/sample - loss: 1.5537 - acc: 0.4627\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 4s 76us/sample - loss: 1.5404 - acc: 0.4680\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 3s 67us/sample - loss: 1.5284 - acc: 0.4738\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 4s 74us/sample - loss: 1.5175 - acc: 0.4773\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 4s 73us/sample - loss: 1.5052 - acc: 0.4825\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 3s 70us/sample - loss: 1.4970 - acc: 0.4846\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 1.6557 - acc: 0.4256\n",
            "\n",
            "Test accuracy: 42.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGMJmSSMqOjG",
        "colab_type": "text"
      },
      "source": [
        "#### Summary\n",
        "Just as discussed, adam really performs better compared to others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4C6brsmptaL",
        "colab_type": "text"
      },
      "source": [
        "### Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyLFePHCuGdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vnaAxvbtWAa",
        "colab_type": "code",
        "outputId": "979ac838-f7b8-4a02-a2f2-37c941b13c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "\n",
        "# simple early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "\n",
        "model.fit(x_train_gray, y_train, batch_size=batch_size, nb_epoch=300,\n",
        "      shuffle=True, verbose=1, callbacks=[es], validation_split=0.33)\n",
        "\n",
        "\n",
        "# model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_81 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_81 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_82 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_83 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 33500 samples, validate on 16500 samples\n",
            "Epoch 1/300\n",
            "33500/33500 [==============================] - 4s 116us/sample - loss: 2.1096 - acc: 0.2505 - val_loss: 1.9978 - val_acc: 0.3070\n",
            "Epoch 2/300\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.9511 - acc: 0.3164 - val_loss: 1.9403 - val_acc: 0.3228\n",
            "Epoch 3/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.8851 - acc: 0.3420 - val_loss: 1.9002 - val_acc: 0.3393\n",
            "Epoch 4/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.8374 - acc: 0.3592 - val_loss: 1.8534 - val_acc: 0.3493\n",
            "Epoch 5/300\n",
            "33500/33500 [==============================] - 3s 79us/sample - loss: 1.8021 - acc: 0.3691 - val_loss: 1.8086 - val_acc: 0.3698\n",
            "Epoch 6/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 1.7733 - acc: 0.3820 - val_loss: 1.8326 - val_acc: 0.3638\n",
            "Epoch 00006: early stopping\n",
            "10000/10000 [==============================] - 0s 34us/sample - loss: 1.8225 - acc: 0.3624\n",
            "\n",
            "Test accuracy: 36.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4RSycDYvq3t",
        "colab_type": "text"
      },
      "source": [
        "It stopped prematurely, let's add patience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REKyAtmWuAXE",
        "colab_type": "code",
        "outputId": "7beca643-8495-4866-87a8-0a6a7d1b8f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "\n",
        "# simple early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "\n",
        "model.fit(x_train_gray, y_train, batch_size=batch_size, nb_epoch=300,\n",
        "      shuffle=True, verbose=1, callbacks=[es], validation_split=0.33)\n",
        "\n",
        "\n",
        "# model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_84 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_84 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_85 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_86 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 33500 samples, validate on 16500 samples\n",
            "Epoch 1/300\n",
            "33500/33500 [==============================] - 4s 109us/sample - loss: 2.1146 - acc: 0.2500 - val_loss: 2.0096 - val_acc: 0.2958\n",
            "Epoch 2/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.9559 - acc: 0.3130 - val_loss: 1.9173 - val_acc: 0.3309\n",
            "Epoch 3/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 1.8858 - acc: 0.3433 - val_loss: 1.8781 - val_acc: 0.3502\n",
            "Epoch 4/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.8470 - acc: 0.3546 - val_loss: 1.8459 - val_acc: 0.3582\n",
            "Epoch 5/300\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.8129 - acc: 0.3667 - val_loss: 1.8531 - val_acc: 0.3523\n",
            "Epoch 6/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.7808 - acc: 0.3768 - val_loss: 1.8414 - val_acc: 0.3589\n",
            "Epoch 7/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.7526 - acc: 0.3890 - val_loss: 1.7795 - val_acc: 0.3808\n",
            "Epoch 8/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 1.7319 - acc: 0.3989 - val_loss: 1.7935 - val_acc: 0.3778\n",
            "Epoch 9/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 1.7062 - acc: 0.4030 - val_loss: 1.7845 - val_acc: 0.3773\n",
            "Epoch 10/300\n",
            "33500/33500 [==============================] - 3s 79us/sample - loss: 1.6829 - acc: 0.4121 - val_loss: 1.7722 - val_acc: 0.3848\n",
            "Epoch 11/300\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.6602 - acc: 0.4249 - val_loss: 1.7620 - val_acc: 0.3939\n",
            "Epoch 12/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 1.6444 - acc: 0.4293 - val_loss: 1.7391 - val_acc: 0.3981\n",
            "Epoch 13/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 1.6237 - acc: 0.4357 - val_loss: 1.7500 - val_acc: 0.3915\n",
            "Epoch 14/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 1.6095 - acc: 0.4419 - val_loss: 1.7275 - val_acc: 0.4031\n",
            "Epoch 15/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.5922 - acc: 0.4475 - val_loss: 1.7302 - val_acc: 0.4020\n",
            "Epoch 16/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.5747 - acc: 0.4548 - val_loss: 1.7348 - val_acc: 0.3974\n",
            "Epoch 17/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 1.5619 - acc: 0.4618 - val_loss: 1.7316 - val_acc: 0.4078\n",
            "Epoch 18/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.5428 - acc: 0.4664 - val_loss: 1.7122 - val_acc: 0.4082\n",
            "Epoch 19/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.5320 - acc: 0.4713 - val_loss: 1.7142 - val_acc: 0.4119\n",
            "Epoch 20/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 1.5203 - acc: 0.4758 - val_loss: 1.6943 - val_acc: 0.4156\n",
            "Epoch 21/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 1.5084 - acc: 0.4808 - val_loss: 1.6856 - val_acc: 0.4252\n",
            "Epoch 22/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 1.4951 - acc: 0.4910 - val_loss: 1.6961 - val_acc: 0.4226\n",
            "Epoch 23/300\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.4752 - acc: 0.4954 - val_loss: 1.7206 - val_acc: 0.4170\n",
            "Epoch 24/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 1.4710 - acc: 0.4973 - val_loss: 1.7327 - val_acc: 0.4146\n",
            "Epoch 25/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.4647 - acc: 0.5003 - val_loss: 1.7046 - val_acc: 0.4226\n",
            "Epoch 26/300\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.4446 - acc: 0.5077 - val_loss: 1.6917 - val_acc: 0.4276\n",
            "Epoch 27/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.4390 - acc: 0.5089 - val_loss: 1.6974 - val_acc: 0.4264\n",
            "Epoch 28/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 1.4236 - acc: 0.5171 - val_loss: 1.7252 - val_acc: 0.4209\n",
            "Epoch 29/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 1.4160 - acc: 0.5170 - val_loss: 1.7225 - val_acc: 0.4182\n",
            "Epoch 30/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 1.4036 - acc: 0.5219 - val_loss: 1.7496 - val_acc: 0.4181\n",
            "Epoch 31/300\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.3908 - acc: 0.5295 - val_loss: 1.7395 - val_acc: 0.4251\n",
            "Epoch 32/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.3858 - acc: 0.5321 - val_loss: 1.7354 - val_acc: 0.4252\n",
            "Epoch 33/300\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.3733 - acc: 0.5370 - val_loss: 1.7577 - val_acc: 0.4198\n",
            "Epoch 34/300\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.3683 - acc: 0.5371 - val_loss: 1.7459 - val_acc: 0.4211\n",
            "Epoch 35/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 1.3569 - acc: 0.5413 - val_loss: 1.7506 - val_acc: 0.4218\n",
            "Epoch 36/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.3509 - acc: 0.5438 - val_loss: 1.7704 - val_acc: 0.4162\n",
            "Epoch 37/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.3349 - acc: 0.5525 - val_loss: 1.7646 - val_acc: 0.4275\n",
            "Epoch 38/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 1.3340 - acc: 0.5529 - val_loss: 1.7822 - val_acc: 0.4245\n",
            "Epoch 39/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.3256 - acc: 0.5550 - val_loss: 1.7713 - val_acc: 0.4232\n",
            "Epoch 40/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.3132 - acc: 0.5580 - val_loss: 1.7715 - val_acc: 0.4290\n",
            "Epoch 41/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 1.3070 - acc: 0.5614 - val_loss: 1.8020 - val_acc: 0.4193\n",
            "Epoch 42/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.2988 - acc: 0.5674 - val_loss: 1.7858 - val_acc: 0.4282\n",
            "Epoch 43/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.2825 - acc: 0.5750 - val_loss: 1.8107 - val_acc: 0.4243\n",
            "Epoch 44/300\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.2840 - acc: 0.5736 - val_loss: 1.8189 - val_acc: 0.4195\n",
            "Epoch 45/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.2797 - acc: 0.5752 - val_loss: 1.8426 - val_acc: 0.4184\n",
            "Epoch 46/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 1.2639 - acc: 0.5828 - val_loss: 1.8251 - val_acc: 0.4158\n",
            "Epoch 47/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.2642 - acc: 0.5804 - val_loss: 1.8547 - val_acc: 0.4178\n",
            "Epoch 48/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 1.2555 - acc: 0.5851 - val_loss: 1.8370 - val_acc: 0.4192\n",
            "Epoch 49/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.2556 - acc: 0.5853 - val_loss: 1.8743 - val_acc: 0.4143\n",
            "Epoch 50/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 1.2397 - acc: 0.5924 - val_loss: 1.8433 - val_acc: 0.4272\n",
            "Epoch 51/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 1.2389 - acc: 0.5940 - val_loss: 1.8887 - val_acc: 0.4195\n",
            "Epoch 52/300\n",
            "33500/33500 [==============================] - 3s 78us/sample - loss: 1.2247 - acc: 0.5984 - val_loss: 1.8781 - val_acc: 0.4250\n",
            "Epoch 53/300\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.2275 - acc: 0.5965 - val_loss: 1.8838 - val_acc: 0.4250\n",
            "Epoch 54/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 1.2209 - acc: 0.5985 - val_loss: 1.8941 - val_acc: 0.4173\n",
            "Epoch 55/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 1.2120 - acc: 0.6044 - val_loss: 1.8735 - val_acc: 0.4268\n",
            "Epoch 56/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 1.2044 - acc: 0.6061 - val_loss: 1.8816 - val_acc: 0.4222\n",
            "Epoch 57/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 1.1918 - acc: 0.6141 - val_loss: 1.9054 - val_acc: 0.4148\n",
            "Epoch 58/300\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.1851 - acc: 0.6153 - val_loss: 1.9030 - val_acc: 0.4213\n",
            "Epoch 59/300\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.1808 - acc: 0.6171 - val_loss: 1.9011 - val_acc: 0.4249\n",
            "Epoch 60/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 1.1771 - acc: 0.6175 - val_loss: 1.9254 - val_acc: 0.4171\n",
            "Epoch 61/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.1679 - acc: 0.6198 - val_loss: 1.9225 - val_acc: 0.4188\n",
            "Epoch 62/300\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.1695 - acc: 0.6218 - val_loss: 1.9354 - val_acc: 0.4250\n",
            "Epoch 63/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 1.1626 - acc: 0.6250 - val_loss: 1.9510 - val_acc: 0.4207\n",
            "Epoch 64/300\n",
            "33500/33500 [==============================] - 3s 83us/sample - loss: 1.1504 - acc: 0.6300 - val_loss: 1.9890 - val_acc: 0.4189\n",
            "Epoch 65/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 1.1576 - acc: 0.6261 - val_loss: 1.9870 - val_acc: 0.4121\n",
            "Epoch 66/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 1.1407 - acc: 0.6358 - val_loss: 1.9709 - val_acc: 0.4183\n",
            "Epoch 67/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.1404 - acc: 0.6322 - val_loss: 2.0247 - val_acc: 0.4084\n",
            "Epoch 68/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.1379 - acc: 0.6363 - val_loss: 2.0152 - val_acc: 0.4131\n",
            "Epoch 69/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 1.1260 - acc: 0.6394 - val_loss: 2.0120 - val_acc: 0.4112\n",
            "Epoch 70/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.1203 - acc: 0.6398 - val_loss: 2.0082 - val_acc: 0.4152\n",
            "Epoch 71/300\n",
            "33500/33500 [==============================] - 3s 83us/sample - loss: 1.1195 - acc: 0.6444 - val_loss: 2.0077 - val_acc: 0.4184\n",
            "Epoch 72/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.1053 - acc: 0.6471 - val_loss: 2.0267 - val_acc: 0.4140\n",
            "Epoch 73/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.0979 - acc: 0.6515 - val_loss: 2.0302 - val_acc: 0.4162\n",
            "Epoch 74/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 1.1003 - acc: 0.6492 - val_loss: 2.0500 - val_acc: 0.4161\n",
            "Epoch 75/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.1013 - acc: 0.6521 - val_loss: 2.0707 - val_acc: 0.4088\n",
            "Epoch 76/300\n",
            "33500/33500 [==============================] - 3s 78us/sample - loss: 1.0927 - acc: 0.6544 - val_loss: 2.0835 - val_acc: 0.4094\n",
            "Epoch 77/300\n",
            "33500/33500 [==============================] - 3s 83us/sample - loss: 1.0915 - acc: 0.6549 - val_loss: 2.0805 - val_acc: 0.4088\n",
            "Epoch 78/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 1.0850 - acc: 0.6561 - val_loss: 2.0940 - val_acc: 0.4155\n",
            "Epoch 79/300\n",
            "33500/33500 [==============================] - 4s 107us/sample - loss: 1.0801 - acc: 0.6572 - val_loss: 2.0894 - val_acc: 0.4128\n",
            "Epoch 80/300\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.0663 - acc: 0.6643 - val_loss: 2.0860 - val_acc: 0.4148\n",
            "Epoch 81/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.0682 - acc: 0.6661 - val_loss: 2.1122 - val_acc: 0.4066\n",
            "Epoch 82/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.0638 - acc: 0.6666 - val_loss: 2.1193 - val_acc: 0.4101\n",
            "Epoch 83/300\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 1.0668 - acc: 0.6674 - val_loss: 2.1525 - val_acc: 0.4068\n",
            "Epoch 84/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 1.0576 - acc: 0.6693 - val_loss: 2.1456 - val_acc: 0.4127\n",
            "Epoch 85/300\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.0568 - acc: 0.6668 - val_loss: 2.1870 - val_acc: 0.4010\n",
            "Epoch 86/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.0491 - acc: 0.6725 - val_loss: 2.1695 - val_acc: 0.4121\n",
            "Epoch 87/300\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.0453 - acc: 0.6729 - val_loss: 2.1527 - val_acc: 0.4150\n",
            "Epoch 88/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.0481 - acc: 0.6738 - val_loss: 2.1632 - val_acc: 0.4084\n",
            "Epoch 89/300\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.0229 - acc: 0.6846 - val_loss: 2.2476 - val_acc: 0.4024\n",
            "Epoch 90/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.0312 - acc: 0.6805 - val_loss: 2.1780 - val_acc: 0.4108\n",
            "Epoch 91/300\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.0279 - acc: 0.6827 - val_loss: 2.2119 - val_acc: 0.4041\n",
            "Epoch 92/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.0287 - acc: 0.6829 - val_loss: 2.2178 - val_acc: 0.4074\n",
            "Epoch 93/300\n",
            "33500/33500 [==============================] - 3s 83us/sample - loss: 1.0207 - acc: 0.6870 - val_loss: 2.2570 - val_acc: 0.4034\n",
            "Epoch 94/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 1.0145 - acc: 0.6883 - val_loss: 2.2679 - val_acc: 0.4035\n",
            "Epoch 95/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 1.0244 - acc: 0.6851 - val_loss: 2.2203 - val_acc: 0.4015\n",
            "Epoch 96/300\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 1.0120 - acc: 0.6888 - val_loss: 2.2572 - val_acc: 0.4065\n",
            "Epoch 97/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.0104 - acc: 0.6917 - val_loss: 2.2642 - val_acc: 0.4005\n",
            "Epoch 98/300\n",
            "33500/33500 [==============================] - 3s 83us/sample - loss: 1.0092 - acc: 0.6911 - val_loss: 2.2838 - val_acc: 0.4051\n",
            "Epoch 99/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 1.0094 - acc: 0.6891 - val_loss: 2.2709 - val_acc: 0.4043\n",
            "Epoch 100/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 0.9890 - acc: 0.6996 - val_loss: 2.2573 - val_acc: 0.4096\n",
            "Epoch 101/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 0.9909 - acc: 0.6979 - val_loss: 2.2700 - val_acc: 0.4082\n",
            "Epoch 102/300\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 0.9861 - acc: 0.7004 - val_loss: 2.3029 - val_acc: 0.4012\n",
            "Epoch 103/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 0.9851 - acc: 0.6983 - val_loss: 2.2888 - val_acc: 0.4042\n",
            "Epoch 104/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 0.9780 - acc: 0.7019 - val_loss: 2.3130 - val_acc: 0.4044\n",
            "Epoch 105/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 0.9836 - acc: 0.7003 - val_loss: 2.3253 - val_acc: 0.4002\n",
            "Epoch 106/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 0.9794 - acc: 0.7031 - val_loss: 2.3571 - val_acc: 0.4053\n",
            "Epoch 107/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.9773 - acc: 0.7024 - val_loss: 2.3961 - val_acc: 0.3924\n",
            "Epoch 108/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 0.9764 - acc: 0.7072 - val_loss: 2.3995 - val_acc: 0.3940\n",
            "Epoch 109/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 0.9654 - acc: 0.7087 - val_loss: 2.3543 - val_acc: 0.4065\n",
            "Epoch 110/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 0.9626 - acc: 0.7109 - val_loss: 2.3541 - val_acc: 0.4087\n",
            "Epoch 111/300\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 0.9457 - acc: 0.7179 - val_loss: 2.3796 - val_acc: 0.4005\n",
            "Epoch 112/300\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 0.9551 - acc: 0.7132 - val_loss: 2.4012 - val_acc: 0.4036\n",
            "Epoch 113/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 0.9534 - acc: 0.7150 - val_loss: 2.3819 - val_acc: 0.4062\n",
            "Epoch 114/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 0.9447 - acc: 0.7163 - val_loss: 2.4600 - val_acc: 0.3994\n",
            "Epoch 115/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.9505 - acc: 0.7159 - val_loss: 2.4317 - val_acc: 0.4033\n",
            "Epoch 116/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 0.9402 - acc: 0.7200 - val_loss: 2.4236 - val_acc: 0.4006\n",
            "Epoch 117/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.9558 - acc: 0.7141 - val_loss: 2.4523 - val_acc: 0.3982\n",
            "Epoch 118/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.9410 - acc: 0.7230 - val_loss: 2.4526 - val_acc: 0.3995\n",
            "Epoch 119/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.9286 - acc: 0.7229 - val_loss: 2.4978 - val_acc: 0.3943\n",
            "Epoch 120/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 0.9277 - acc: 0.7240 - val_loss: 2.5032 - val_acc: 0.3973\n",
            "Epoch 121/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 0.9322 - acc: 0.7246 - val_loss: 2.4606 - val_acc: 0.4016\n",
            "Epoch 122/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 0.9318 - acc: 0.7224 - val_loss: 2.5035 - val_acc: 0.3959\n",
            "Epoch 123/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 0.9201 - acc: 0.7283 - val_loss: 2.4945 - val_acc: 0.4003\n",
            "Epoch 124/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 0.9306 - acc: 0.7245 - val_loss: 2.5322 - val_acc: 0.3883\n",
            "Epoch 125/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 0.9230 - acc: 0.7276 - val_loss: 2.5258 - val_acc: 0.3924\n",
            "Epoch 126/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 0.9124 - acc: 0.7345 - val_loss: 2.5369 - val_acc: 0.3967\n",
            "Epoch 127/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 0.9143 - acc: 0.7330 - val_loss: 2.5014 - val_acc: 0.4033\n",
            "Epoch 128/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 0.9241 - acc: 0.7292 - val_loss: 2.5540 - val_acc: 0.3988\n",
            "Epoch 129/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.9041 - acc: 0.7373 - val_loss: 2.5686 - val_acc: 0.3931\n",
            "Epoch 130/300\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 0.9076 - acc: 0.7343 - val_loss: 2.5303 - val_acc: 0.4015\n",
            "Epoch 131/300\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 0.8916 - acc: 0.7402 - val_loss: 2.6073 - val_acc: 0.3956\n",
            "Epoch 132/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 0.8935 - acc: 0.7429 - val_loss: 2.5822 - val_acc: 0.3967\n",
            "Epoch 133/300\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 0.8882 - acc: 0.7429 - val_loss: 2.5543 - val_acc: 0.4018\n",
            "Epoch 134/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 0.8797 - acc: 0.7479 - val_loss: 2.6027 - val_acc: 0.3967\n",
            "Epoch 135/300\n",
            "33500/33500 [==============================] - 4s 105us/sample - loss: 0.8886 - acc: 0.7404 - val_loss: 2.6477 - val_acc: 0.3933\n",
            "Epoch 136/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 0.8986 - acc: 0.7395 - val_loss: 2.6747 - val_acc: 0.3973\n",
            "Epoch 137/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 0.8975 - acc: 0.7396 - val_loss: 2.6852 - val_acc: 0.3889\n",
            "Epoch 138/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 0.9048 - acc: 0.7337 - val_loss: 2.6369 - val_acc: 0.3912\n",
            "Epoch 139/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 0.8903 - acc: 0.7424 - val_loss: 2.6583 - val_acc: 0.3936\n",
            "Epoch 140/300\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 0.8835 - acc: 0.7462 - val_loss: 2.6628 - val_acc: 0.3920\n",
            "Epoch 141/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 0.8753 - acc: 0.7471 - val_loss: 2.6879 - val_acc: 0.3927\n",
            "Epoch 142/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 0.8884 - acc: 0.7430 - val_loss: 2.6701 - val_acc: 0.3949\n",
            "Epoch 143/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 0.8726 - acc: 0.7508 - val_loss: 2.7280 - val_acc: 0.3905\n",
            "Epoch 144/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 0.8657 - acc: 0.7545 - val_loss: 2.7735 - val_acc: 0.3945\n",
            "Epoch 145/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.8715 - acc: 0.7497 - val_loss: 2.6892 - val_acc: 0.3965\n",
            "Epoch 146/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.8674 - acc: 0.7511 - val_loss: 2.7418 - val_acc: 0.3925\n",
            "Epoch 147/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.8580 - acc: 0.7572 - val_loss: 2.7410 - val_acc: 0.3855\n",
            "Epoch 148/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.8712 - acc: 0.7516 - val_loss: 2.7214 - val_acc: 0.3880\n",
            "Epoch 149/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 0.8577 - acc: 0.7562 - val_loss: 2.7460 - val_acc: 0.3910\n",
            "Epoch 150/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 0.8653 - acc: 0.7535 - val_loss: 2.7666 - val_acc: 0.3862\n",
            "Epoch 151/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 0.8581 - acc: 0.7564 - val_loss: 2.7707 - val_acc: 0.3886\n",
            "Epoch 152/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 0.8473 - acc: 0.7611 - val_loss: 2.7725 - val_acc: 0.3959\n",
            "Epoch 153/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 0.8516 - acc: 0.7572 - val_loss: 2.8022 - val_acc: 0.3845\n",
            "Epoch 154/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 0.8446 - acc: 0.7622 - val_loss: 2.8099 - val_acc: 0.3904\n",
            "Epoch 155/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.8536 - acc: 0.7583 - val_loss: 2.7518 - val_acc: 0.3952\n",
            "Epoch 156/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 0.8518 - acc: 0.7575 - val_loss: 2.8239 - val_acc: 0.3912\n",
            "Epoch 157/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.8551 - acc: 0.7567 - val_loss: 2.8161 - val_acc: 0.3865\n",
            "Epoch 158/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 0.8618 - acc: 0.7583 - val_loss: 2.8608 - val_acc: 0.3860\n",
            "Epoch 159/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 0.8419 - acc: 0.7654 - val_loss: 2.7947 - val_acc: 0.3975\n",
            "Epoch 160/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 0.8321 - acc: 0.7691 - val_loss: 2.8669 - val_acc: 0.3859\n",
            "Epoch 161/300\n",
            "33500/33500 [==============================] - 3s 83us/sample - loss: 0.8295 - acc: 0.7691 - val_loss: 2.8496 - val_acc: 0.3853\n",
            "Epoch 162/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.8407 - acc: 0.7637 - val_loss: 2.8656 - val_acc: 0.3923\n",
            "Epoch 163/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 0.8224 - acc: 0.7716 - val_loss: 2.9006 - val_acc: 0.3968\n",
            "Epoch 164/300\n",
            "33500/33500 [==============================] - 3s 79us/sample - loss: 0.8503 - acc: 0.7583 - val_loss: 2.8620 - val_acc: 0.3935\n",
            "Epoch 165/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 0.8173 - acc: 0.7739 - val_loss: 2.8905 - val_acc: 0.3928\n",
            "Epoch 166/300\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 0.8190 - acc: 0.7722 - val_loss: 2.9276 - val_acc: 0.3876\n",
            "Epoch 167/300\n",
            "33500/33500 [==============================] - 3s 79us/sample - loss: 0.8304 - acc: 0.7697 - val_loss: 2.9338 - val_acc: 0.3815\n",
            "Epoch 168/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.8337 - acc: 0.7698 - val_loss: 2.8631 - val_acc: 0.3940\n",
            "Epoch 169/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 0.8230 - acc: 0.7731 - val_loss: 2.9855 - val_acc: 0.3798\n",
            "Epoch 170/300\n",
            "33500/33500 [==============================] - 3s 78us/sample - loss: 0.8367 - acc: 0.7688 - val_loss: 2.9418 - val_acc: 0.3902\n",
            "Epoch 171/300\n",
            "33500/33500 [==============================] - 3s 79us/sample - loss: 0.8094 - acc: 0.7793 - val_loss: 2.9372 - val_acc: 0.3939\n",
            "Epoch 172/300\n",
            "33500/33500 [==============================] - 3s 78us/sample - loss: 0.8094 - acc: 0.7775 - val_loss: 2.9828 - val_acc: 0.3906\n",
            "Epoch 173/300\n",
            "33500/33500 [==============================] - 3s 78us/sample - loss: 0.8176 - acc: 0.7761 - val_loss: 2.9382 - val_acc: 0.3948\n",
            "Epoch 174/300\n",
            "33500/33500 [==============================] - 3s 78us/sample - loss: 0.8078 - acc: 0.7787 - val_loss: 2.9587 - val_acc: 0.3892\n",
            "Epoch 175/300\n",
            "33500/33500 [==============================] - 3s 83us/sample - loss: 0.8099 - acc: 0.7797 - val_loss: 2.9901 - val_acc: 0.3892\n",
            "Epoch 176/300\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 0.8190 - acc: 0.7758 - val_loss: 3.0258 - val_acc: 0.3874\n",
            "Epoch 177/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 0.8221 - acc: 0.7748 - val_loss: 2.9940 - val_acc: 0.3940\n",
            "Epoch 178/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.8071 - acc: 0.7802 - val_loss: 3.0031 - val_acc: 0.3899\n",
            "Epoch 179/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.8040 - acc: 0.7807 - val_loss: 3.0104 - val_acc: 0.3886\n",
            "Epoch 180/300\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 0.7948 - acc: 0.7849 - val_loss: 3.0109 - val_acc: 0.3853\n",
            "Epoch 181/300\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 0.8142 - acc: 0.7779 - val_loss: 3.0144 - val_acc: 0.3921\n",
            "Epoch 182/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 0.7978 - acc: 0.7822 - val_loss: 2.9981 - val_acc: 0.3876\n",
            "Epoch 183/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.7890 - acc: 0.7879 - val_loss: 3.0946 - val_acc: 0.3859\n",
            "Epoch 184/300\n",
            "33500/33500 [==============================] - 3s 104us/sample - loss: 0.8079 - acc: 0.7769 - val_loss: 3.0961 - val_acc: 0.3850\n",
            "Epoch 185/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.8007 - acc: 0.7829 - val_loss: 3.0746 - val_acc: 0.3878\n",
            "Epoch 186/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 0.7950 - acc: 0.7852 - val_loss: 3.0610 - val_acc: 0.3912\n",
            "Epoch 187/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 0.7971 - acc: 0.7847 - val_loss: 3.0422 - val_acc: 0.3826\n",
            "Epoch 188/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 0.7797 - acc: 0.7931 - val_loss: 3.0732 - val_acc: 0.3901\n",
            "Epoch 189/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 0.7930 - acc: 0.7861 - val_loss: 3.0917 - val_acc: 0.3838\n",
            "Epoch 190/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 0.8116 - acc: 0.7781 - val_loss: 3.1188 - val_acc: 0.3908\n",
            "Epoch 191/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 0.7911 - acc: 0.7882 - val_loss: 3.1825 - val_acc: 0.3814\n",
            "Epoch 192/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.7955 - acc: 0.7861 - val_loss: 3.1031 - val_acc: 0.3884\n",
            "Epoch 193/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.7939 - acc: 0.7854 - val_loss: 3.1842 - val_acc: 0.3828\n",
            "Epoch 194/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 0.7894 - acc: 0.7870 - val_loss: 3.1246 - val_acc: 0.3900\n",
            "Epoch 195/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 0.7886 - acc: 0.7886 - val_loss: 3.1123 - val_acc: 0.3855\n",
            "Epoch 196/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 0.7843 - acc: 0.7923 - val_loss: 3.1705 - val_acc: 0.3837\n",
            "Epoch 197/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 0.7826 - acc: 0.7905 - val_loss: 3.1648 - val_acc: 0.3882\n",
            "Epoch 198/300\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 0.7774 - acc: 0.7941 - val_loss: 3.2528 - val_acc: 0.3884\n",
            "Epoch 199/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 0.7855 - acc: 0.7901 - val_loss: 3.2033 - val_acc: 0.3790\n",
            "Epoch 200/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 0.7786 - acc: 0.7942 - val_loss: 3.1435 - val_acc: 0.3910\n",
            "Epoch 201/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 0.7730 - acc: 0.7944 - val_loss: 3.2075 - val_acc: 0.3776\n",
            "Epoch 202/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 0.7684 - acc: 0.7975 - val_loss: 3.2099 - val_acc: 0.3832\n",
            "Epoch 203/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.7676 - acc: 0.7957 - val_loss: 3.2249 - val_acc: 0.3855\n",
            "Epoch 204/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 0.7878 - acc: 0.7895 - val_loss: 3.1930 - val_acc: 0.3863\n",
            "Epoch 205/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 0.7636 - acc: 0.7986 - val_loss: 3.2742 - val_acc: 0.3837\n",
            "Epoch 206/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 0.7725 - acc: 0.7975 - val_loss: 3.2020 - val_acc: 0.3908\n",
            "Epoch 207/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 0.7791 - acc: 0.7936 - val_loss: 3.2581 - val_acc: 0.3857\n",
            "Epoch 208/300\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 0.7657 - acc: 0.7976 - val_loss: 3.2300 - val_acc: 0.3812\n",
            "Epoch 209/300\n",
            "33500/33500 [==============================] - 3s 79us/sample - loss: 0.7842 - acc: 0.7913 - val_loss: 3.2521 - val_acc: 0.3824\n",
            "Epoch 210/300\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 0.7677 - acc: 0.7991 - val_loss: 3.2533 - val_acc: 0.3875\n",
            "Epoch 211/300\n",
            "33500/33500 [==============================] - 3s 83us/sample - loss: 0.7534 - acc: 0.8059 - val_loss: 3.2686 - val_acc: 0.3734\n",
            "Epoch 212/300\n",
            "33500/33500 [==============================] - 3s 79us/sample - loss: 0.7400 - acc: 0.8095 - val_loss: 3.2745 - val_acc: 0.3860\n",
            "Epoch 213/300\n",
            "33500/33500 [==============================] - 3s 78us/sample - loss: 0.7412 - acc: 0.8099 - val_loss: 3.3158 - val_acc: 0.3768\n",
            "Epoch 214/300\n",
            "33500/33500 [==============================] - 3s 78us/sample - loss: 0.7629 - acc: 0.7999 - val_loss: 3.3835 - val_acc: 0.3796\n",
            "Epoch 215/300\n",
            "33500/33500 [==============================] - 3s 79us/sample - loss: 0.7770 - acc: 0.7954 - val_loss: 3.3354 - val_acc: 0.3839\n",
            "Epoch 216/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 0.7728 - acc: 0.7966 - val_loss: 3.3094 - val_acc: 0.3845\n",
            "Epoch 217/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 0.7535 - acc: 0.8064 - val_loss: 3.3595 - val_acc: 0.3812\n",
            "Epoch 218/300\n",
            "33500/33500 [==============================] - 3s 83us/sample - loss: 0.7806 - acc: 0.7945 - val_loss: 3.3528 - val_acc: 0.3816\n",
            "Epoch 219/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 0.7642 - acc: 0.8013 - val_loss: 3.3688 - val_acc: 0.3807\n",
            "Epoch 220/300\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 0.7453 - acc: 0.8091 - val_loss: 3.3175 - val_acc: 0.3833\n",
            "Epoch 221/300\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 0.7554 - acc: 0.8041 - val_loss: 3.3669 - val_acc: 0.3842\n",
            "Epoch 00221: early stopping\n",
            "10000/10000 [==============================] - 0s 38us/sample - loss: 3.3725 - acc: 0.3805\n",
            "\n",
            "Test accuracy: 38.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uIBT1m3GWa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_gray[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC_VJl8_vwQB",
        "colab_type": "text"
      },
      "source": [
        "The training accuracy continuously increase, however the validation accuracy gets lower and lower after 34 epochs. \n",
        "And it doesnt stop even if validation accuracy is getting lower.\n",
        "\n",
        "Make patience small"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpB9asfAxLaH",
        "colab_type": "code",
        "outputId": "8e0eff23-de1d-4d97-d485-3df3540040aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "\n",
        "# simple early stopping\n",
        "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n",
        "\n",
        "model.fit(x_train_gray, y_train, batch_size=batch_size, nb_epoch=300,\n",
        "      shuffle=True, verbose=1, callbacks=[es], validation_split=0.33)\n",
        "\n",
        "\n",
        "# model.fit(x_train_gray, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_87 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_87 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_88 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_89 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 33500 samples, validate on 16500 samples\n",
            "Epoch 1/300\n",
            "33500/33500 [==============================] - 4s 118us/sample - loss: 2.1156 - acc: 0.2463 - val_loss: 2.0041 - val_acc: 0.2822\n",
            "Epoch 2/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.9554 - acc: 0.3113 - val_loss: 1.9311 - val_acc: 0.3181\n",
            "Epoch 3/300\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.8800 - acc: 0.3410 - val_loss: 1.8900 - val_acc: 0.3280\n",
            "Epoch 4/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.8309 - acc: 0.3582 - val_loss: 1.8354 - val_acc: 0.3559\n",
            "Epoch 5/300\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.7969 - acc: 0.3717 - val_loss: 1.8165 - val_acc: 0.3652\n",
            "Epoch 6/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.7627 - acc: 0.3871 - val_loss: 1.8056 - val_acc: 0.3738\n",
            "Epoch 7/300\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.7379 - acc: 0.3931 - val_loss: 1.7972 - val_acc: 0.3724\n",
            "Epoch 8/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 1.7129 - acc: 0.3999 - val_loss: 1.7720 - val_acc: 0.3782\n",
            "Epoch 9/300\n",
            "33500/33500 [==============================] - 3s 84us/sample - loss: 1.6912 - acc: 0.4105 - val_loss: 1.7359 - val_acc: 0.3972\n",
            "Epoch 10/300\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 1.6712 - acc: 0.4170 - val_loss: 1.7462 - val_acc: 0.3932\n",
            "Epoch 11/300\n",
            "33500/33500 [==============================] - 4s 106us/sample - loss: 1.6439 - acc: 0.4283 - val_loss: 1.7448 - val_acc: 0.3942\n",
            "Epoch 12/300\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 1.6306 - acc: 0.4326 - val_loss: 1.7100 - val_acc: 0.4032\n",
            "Epoch 13/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 1.6137 - acc: 0.4384 - val_loss: 1.7114 - val_acc: 0.4015\n",
            "Epoch 14/300\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 1.5988 - acc: 0.4455 - val_loss: 1.7363 - val_acc: 0.3928\n",
            "Epoch 15/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 1.5839 - acc: 0.4506 - val_loss: 1.7205 - val_acc: 0.4003\n",
            "Epoch 16/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 1.5737 - acc: 0.4533 - val_loss: 1.7036 - val_acc: 0.4110\n",
            "Epoch 17/300\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 1.5568 - acc: 0.4610 - val_loss: 1.7283 - val_acc: 0.4014\n",
            "Epoch 18/300\n",
            "33500/33500 [==============================] - 4s 108us/sample - loss: 1.5397 - acc: 0.4705 - val_loss: 1.6747 - val_acc: 0.4192\n",
            "Epoch 19/300\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.5259 - acc: 0.4731 - val_loss: 1.6759 - val_acc: 0.4236\n",
            "Epoch 20/300\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.5155 - acc: 0.4790 - val_loss: 1.7390 - val_acc: 0.4096\n",
            "Epoch 21/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.5030 - acc: 0.4835 - val_loss: 1.7018 - val_acc: 0.4180\n",
            "Epoch 22/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.4963 - acc: 0.4865 - val_loss: 1.7028 - val_acc: 0.4175\n",
            "Epoch 23/300\n",
            "33500/33500 [==============================] - 3s 101us/sample - loss: 1.4797 - acc: 0.4901 - val_loss: 1.6857 - val_acc: 0.4248\n",
            "Epoch 24/300\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.4740 - acc: 0.4939 - val_loss: 1.6750 - val_acc: 0.4244\n",
            "Epoch 25/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.4596 - acc: 0.5012 - val_loss: 1.7112 - val_acc: 0.4221\n",
            "Epoch 26/300\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.4523 - acc: 0.5045 - val_loss: 1.7078 - val_acc: 0.4178\n",
            "Epoch 27/300\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.4375 - acc: 0.5083 - val_loss: 1.7103 - val_acc: 0.4174\n",
            "Epoch 28/300\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.4231 - acc: 0.5113 - val_loss: 1.7091 - val_acc: 0.4270\n",
            "Epoch 29/300\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.4166 - acc: 0.5175 - val_loss: 1.7274 - val_acc: 0.4150\n",
            "Epoch 30/300\n",
            "33500/33500 [==============================] - 4s 105us/sample - loss: 1.4136 - acc: 0.5186 - val_loss: 1.7198 - val_acc: 0.4192\n",
            "Epoch 31/300\n",
            "33500/33500 [==============================] - 3s 104us/sample - loss: 1.3940 - acc: 0.5276 - val_loss: 1.7283 - val_acc: 0.4179\n",
            "Epoch 32/300\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.3944 - acc: 0.5256 - val_loss: 1.7151 - val_acc: 0.4248\n",
            "Epoch 33/300\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.3799 - acc: 0.5346 - val_loss: 1.7271 - val_acc: 0.4241\n",
            "Epoch 34/300\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.3731 - acc: 0.5332 - val_loss: 1.7364 - val_acc: 0.4273\n",
            "Epoch 35/300\n",
            "33500/33500 [==============================] - 3s 101us/sample - loss: 1.3658 - acc: 0.5339 - val_loss: 1.7651 - val_acc: 0.4177\n",
            "Epoch 36/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.3516 - acc: 0.5430 - val_loss: 1.7457 - val_acc: 0.4219\n",
            "Epoch 37/300\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 1.3503 - acc: 0.5430 - val_loss: 1.7352 - val_acc: 0.4247\n",
            "Epoch 38/300\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.3436 - acc: 0.5437 - val_loss: 1.7368 - val_acc: 0.4252\n",
            "Epoch 39/300\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.3294 - acc: 0.5517 - val_loss: 1.7467 - val_acc: 0.4277\n",
            "Epoch 40/300\n",
            "33500/33500 [==============================] - 3s 101us/sample - loss: 1.3252 - acc: 0.5547 - val_loss: 1.7605 - val_acc: 0.4244\n",
            "Epoch 41/300\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.3131 - acc: 0.5597 - val_loss: 1.7544 - val_acc: 0.4239\n",
            "Epoch 42/300\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.3110 - acc: 0.5598 - val_loss: 1.7679 - val_acc: 0.4221\n",
            "Epoch 43/300\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 1.3092 - acc: 0.5593 - val_loss: 1.8018 - val_acc: 0.4190\n",
            "Epoch 44/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.3014 - acc: 0.5628 - val_loss: 1.7594 - val_acc: 0.4285\n",
            "Epoch 45/300\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.2899 - acc: 0.5673 - val_loss: 1.7996 - val_acc: 0.4222\n",
            "Epoch 46/300\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.2796 - acc: 0.5719 - val_loss: 1.7760 - val_acc: 0.4296\n",
            "Epoch 47/300\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.2797 - acc: 0.5752 - val_loss: 1.7955 - val_acc: 0.4197\n",
            "Epoch 48/300\n",
            "33500/33500 [==============================] - 3s 101us/sample - loss: 1.2700 - acc: 0.5760 - val_loss: 1.7858 - val_acc: 0.4273\n",
            "Epoch 49/300\n",
            "33500/33500 [==============================] - 3s 101us/sample - loss: 1.2712 - acc: 0.5799 - val_loss: 1.8031 - val_acc: 0.4213\n",
            "Epoch 50/300\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.2607 - acc: 0.5795 - val_loss: 1.8143 - val_acc: 0.4231\n",
            "Epoch 51/300\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.2435 - acc: 0.5880 - val_loss: 1.8157 - val_acc: 0.4183\n",
            "Epoch 52/300\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 1.2436 - acc: 0.5890 - val_loss: 1.8175 - val_acc: 0.4242\n",
            "Epoch 53/300\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.2362 - acc: 0.5925 - val_loss: 1.8284 - val_acc: 0.4259\n",
            "Epoch 54/300\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.2339 - acc: 0.5934 - val_loss: 1.8449 - val_acc: 0.4222\n",
            "Epoch 55/300\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.2285 - acc: 0.5952 - val_loss: 1.8378 - val_acc: 0.4252\n",
            "Epoch 56/300\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.2201 - acc: 0.5967 - val_loss: 1.8551 - val_acc: 0.4235\n",
            "Epoch 00056: early stopping\n",
            "10000/10000 [==============================] - 0s 38us/sample - loss: 1.8718 - acc: 0.4191\n",
            "\n",
            "Test accuracy: 41.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T61_SfdCyYw_",
        "colab_type": "text"
      },
      "source": [
        "From this, we can see some validation accuracy that is greater than the current validation accuracy. So, lets use a model checkpoint to get the best model of all our training\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS90N_Ce85JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXH4X7pO8y5H",
        "colab_type": "code",
        "outputId": "e4aa2c54-812c-4986-fdc6-0189501c5542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "\n",
        "# simple early stopping\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n",
        "\n",
        "model.fit(x_train_gray, y_train, batch_size=batch_size, nb_epoch=300,\n",
        "      shuffle=True, verbose=1, callbacks=[es, mc], validation_split=0.33)\n",
        "\n",
        "\n",
        "saved_model = load_model('best_model.h5')\n",
        "\n",
        "loss, acc = saved_model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_90 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_90 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_91 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_92 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 330,762\n",
            "Trainable params: 330,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 33500 samples, validate on 16500 samples\n",
            "Epoch 1/300\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 2.1222 - acc: 0.2503\n",
            "Epoch 00001: val_acc improved from -inf to 0.29818, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 6s 175us/sample - loss: 2.1214 - acc: 0.2505 - val_loss: 2.0025 - val_acc: 0.2982\n",
            "Epoch 2/300\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.9528 - acc: 0.3174\n",
            "Epoch 00002: val_acc improved from 0.29818 to 0.32273, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.9521 - acc: 0.3178 - val_loss: 1.9484 - val_acc: 0.3227\n",
            "Epoch 3/300\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.8939 - acc: 0.3373\n",
            "Epoch 00003: val_acc improved from 0.32273 to 0.34582, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.8925 - acc: 0.3376 - val_loss: 1.8853 - val_acc: 0.3458\n",
            "Epoch 4/300\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.8455 - acc: 0.3564\n",
            "Epoch 00004: val_acc improved from 0.34582 to 0.34752, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.8449 - acc: 0.3568 - val_loss: 1.8541 - val_acc: 0.3475\n",
            "Epoch 5/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.8092 - acc: 0.3670\n",
            "Epoch 00005: val_acc improved from 0.34752 to 0.36648, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 104us/sample - loss: 1.8093 - acc: 0.3671 - val_loss: 1.8098 - val_acc: 0.3665\n",
            "Epoch 6/300\n",
            "32768/33500 [============================>.] - ETA: 0s - loss: 1.7746 - acc: 0.3809\n",
            "Epoch 00006: val_acc did not improve from 0.36648\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.7734 - acc: 0.3816 - val_loss: 1.8635 - val_acc: 0.3536\n",
            "Epoch 7/300\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.7463 - acc: 0.3922\n",
            "Epoch 00007: val_acc improved from 0.36648 to 0.38200, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.7461 - acc: 0.3921 - val_loss: 1.7678 - val_acc: 0.3820\n",
            "Epoch 8/300\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7209 - acc: 0.3985\n",
            "Epoch 00008: val_acc did not improve from 0.38200\n",
            "33500/33500 [==============================] - 3s 100us/sample - loss: 1.7209 - acc: 0.3987 - val_loss: 1.7823 - val_acc: 0.3796\n",
            "Epoch 9/300\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.6936 - acc: 0.4092\n",
            "Epoch 00009: val_acc improved from 0.38200 to 0.38552, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.6933 - acc: 0.4094 - val_loss: 1.7582 - val_acc: 0.3855\n",
            "Epoch 10/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.6685 - acc: 0.4197\n",
            "Epoch 00010: val_acc improved from 0.38552 to 0.39933, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 100us/sample - loss: 1.6691 - acc: 0.4195 - val_loss: 1.7252 - val_acc: 0.3993\n",
            "Epoch 11/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.6527 - acc: 0.4262\n",
            "Epoch 00011: val_acc did not improve from 0.39933\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.6531 - acc: 0.4263 - val_loss: 1.7366 - val_acc: 0.3970\n",
            "Epoch 12/300\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.6416 - acc: 0.4327\n",
            "Epoch 00012: val_acc improved from 0.39933 to 0.40000, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.6415 - acc: 0.4330 - val_loss: 1.7461 - val_acc: 0.4000\n",
            "Epoch 13/300\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.6182 - acc: 0.4402\n",
            "Epoch 00013: val_acc did not improve from 0.40000\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 1.6196 - acc: 0.4397 - val_loss: 1.7554 - val_acc: 0.3930\n",
            "Epoch 14/300\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.6007 - acc: 0.4437\n",
            "Epoch 00014: val_acc did not improve from 0.40000\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.6004 - acc: 0.4438 - val_loss: 1.7533 - val_acc: 0.3970\n",
            "Epoch 15/300\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.5843 - acc: 0.4531\n",
            "Epoch 00015: val_acc improved from 0.40000 to 0.40873, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 101us/sample - loss: 1.5846 - acc: 0.4528 - val_loss: 1.7150 - val_acc: 0.4087\n",
            "Epoch 16/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.5669 - acc: 0.4602\n",
            "Epoch 00016: val_acc improved from 0.40873 to 0.41109, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.5669 - acc: 0.4602 - val_loss: 1.7138 - val_acc: 0.4111\n",
            "Epoch 17/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.5548 - acc: 0.4622\n",
            "Epoch 00017: val_acc did not improve from 0.41109\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.5545 - acc: 0.4622 - val_loss: 1.7037 - val_acc: 0.4108\n",
            "Epoch 18/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.5371 - acc: 0.4693\n",
            "Epoch 00018: val_acc did not improve from 0.41109\n",
            "33500/33500 [==============================] - 3s 85us/sample - loss: 1.5365 - acc: 0.4695 - val_loss: 1.7185 - val_acc: 0.4068\n",
            "Epoch 19/300\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.5276 - acc: 0.4732\n",
            "Epoch 00019: val_acc did not improve from 0.41109\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.5275 - acc: 0.4736 - val_loss: 1.7250 - val_acc: 0.4080\n",
            "Epoch 20/300\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.5135 - acc: 0.4785\n",
            "Epoch 00020: val_acc improved from 0.41109 to 0.41333, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 100us/sample - loss: 1.5138 - acc: 0.4783 - val_loss: 1.7190 - val_acc: 0.4133\n",
            "Epoch 21/300\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.4991 - acc: 0.4843\n",
            "Epoch 00021: val_acc improved from 0.41333 to 0.42236, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.4990 - acc: 0.4843 - val_loss: 1.6834 - val_acc: 0.4224\n",
            "Epoch 22/300\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.4950 - acc: 0.4877\n",
            "Epoch 00022: val_acc did not improve from 0.42236\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.4954 - acc: 0.4876 - val_loss: 1.7334 - val_acc: 0.4060\n",
            "Epoch 23/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.4750 - acc: 0.4934\n",
            "Epoch 00023: val_acc did not improve from 0.42236\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.4750 - acc: 0.4933 - val_loss: 1.7230 - val_acc: 0.4088\n",
            "Epoch 24/300\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.4628 - acc: 0.4975\n",
            "Epoch 00024: val_acc did not improve from 0.42236\n",
            "33500/33500 [==============================] - 3s 101us/sample - loss: 1.4631 - acc: 0.4973 - val_loss: 1.7128 - val_acc: 0.4193\n",
            "Epoch 25/300\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.4564 - acc: 0.4988\n",
            "Epoch 00025: val_acc did not improve from 0.42236\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.4576 - acc: 0.4980 - val_loss: 1.7210 - val_acc: 0.4174\n",
            "Epoch 26/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.4425 - acc: 0.5060\n",
            "Epoch 00026: val_acc did not improve from 0.42236\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.4426 - acc: 0.5061 - val_loss: 1.7246 - val_acc: 0.4126\n",
            "Epoch 27/300\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.4343 - acc: 0.5059\n",
            "Epoch 00027: val_acc did not improve from 0.42236\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.4353 - acc: 0.5058 - val_loss: 1.7322 - val_acc: 0.4153\n",
            "Epoch 28/300\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.4226 - acc: 0.5130\n",
            "Epoch 00028: val_acc did not improve from 0.42236\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.4222 - acc: 0.5128 - val_loss: 1.7115 - val_acc: 0.4184\n",
            "Epoch 29/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.4097 - acc: 0.5184\n",
            "Epoch 00029: val_acc improved from 0.42236 to 0.42515, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 101us/sample - loss: 1.4099 - acc: 0.5186 - val_loss: 1.7124 - val_acc: 0.4252\n",
            "Epoch 30/300\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.3991 - acc: 0.5217\n",
            "Epoch 00030: val_acc did not improve from 0.42515\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.3997 - acc: 0.5217 - val_loss: 1.7450 - val_acc: 0.4160\n",
            "Epoch 31/300\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.3911 - acc: 0.5262\n",
            "Epoch 00031: val_acc improved from 0.42515 to 0.42915, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.3909 - acc: 0.5265 - val_loss: 1.7190 - val_acc: 0.4292\n",
            "Epoch 32/300\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.3803 - acc: 0.5308\n",
            "Epoch 00032: val_acc did not improve from 0.42915\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.3808 - acc: 0.5305 - val_loss: 1.7296 - val_acc: 0.4253\n",
            "Epoch 33/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.3771 - acc: 0.5313\n",
            "Epoch 00033: val_acc did not improve from 0.42915\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.3773 - acc: 0.5313 - val_loss: 1.7405 - val_acc: 0.4170\n",
            "Epoch 34/300\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.3650 - acc: 0.5342\n",
            "Epoch 00034: val_acc did not improve from 0.42915\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.3653 - acc: 0.5345 - val_loss: 1.7495 - val_acc: 0.4215\n",
            "Epoch 35/300\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.3520 - acc: 0.5419\n",
            "Epoch 00035: val_acc did not improve from 0.42915\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.3523 - acc: 0.5417 - val_loss: 1.7434 - val_acc: 0.4279\n",
            "Epoch 36/300\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.3479 - acc: 0.5429\n",
            "Epoch 00036: val_acc did not improve from 0.42915\n",
            "33500/33500 [==============================] - 3s 79us/sample - loss: 1.3478 - acc: 0.5428 - val_loss: 1.7460 - val_acc: 0.4235\n",
            "Epoch 37/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.3383 - acc: 0.5491\n",
            "Epoch 00037: val_acc did not improve from 0.42915\n",
            "33500/33500 [==============================] - 3s 78us/sample - loss: 1.3378 - acc: 0.5493 - val_loss: 1.7545 - val_acc: 0.4245\n",
            "Epoch 38/300\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.3315 - acc: 0.5511\n",
            "Epoch 00038: val_acc did not improve from 0.42915\n",
            "33500/33500 [==============================] - 3s 79us/sample - loss: 1.3316 - acc: 0.5510 - val_loss: 1.7555 - val_acc: 0.4204\n",
            "Epoch 39/300\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.3146 - acc: 0.5580\n",
            "Epoch 00039: val_acc did not improve from 0.42915\n",
            "33500/33500 [==============================] - 3s 80us/sample - loss: 1.3149 - acc: 0.5579 - val_loss: 1.7457 - val_acc: 0.4245\n",
            "Epoch 40/300\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.3124 - acc: 0.5589\n",
            "Epoch 00040: val_acc did not improve from 0.42915\n",
            "33500/33500 [==============================] - 3s 81us/sample - loss: 1.3132 - acc: 0.5589 - val_loss: 1.7866 - val_acc: 0.4195\n",
            "Epoch 41/300\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.3082 - acc: 0.5616\n",
            "Epoch 00041: val_acc did not improve from 0.42915\n",
            "33500/33500 [==============================] - 3s 82us/sample - loss: 1.3083 - acc: 0.5613 - val_loss: 1.7598 - val_acc: 0.4271\n",
            "Epoch 00041: early stopping\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "10000/10000 [==============================] - 1s 64us/sample - loss: 1.7017 - acc: 0.4260\n",
            "\n",
            "Test accuracy: 42.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2qOjD0tG4ft",
        "colab_type": "text"
      },
      "source": [
        "Now, this is the best accuracy for MLP based on my experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHS6JcofGjhF",
        "colab_type": "text"
      },
      "source": [
        "#### Complying to the Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqMVtOK6GoMr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cde21d02-1325-4d2d-dd38-a58d7517a05e"
      },
      "source": [
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of adam optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# simple early stopping\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n",
        "\n",
        "model.fit(x_train_gray, y_train, batch_size=batch_size, nb_epoch=100,\n",
        "      shuffle=True, verbose=1, callbacks=[es, mc], validation_split=0.33)\n",
        "\n",
        "\n",
        "saved_model = load_model('best_model.h5')\n",
        "\n",
        "loss, acc = saved_model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_17 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 396,554\n",
            "Trainable params: 396,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 33500 samples, validate on 16500 samples\n",
            "Epoch 1/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 2.1107 - acc: 0.2537\n",
            "Epoch 00001: val_acc improved from -inf to 0.29576, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 4s 125us/sample - loss: 2.1103 - acc: 0.2539 - val_loss: 2.0198 - val_acc: 0.2958\n",
            "Epoch 2/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.9435 - acc: 0.3217\n",
            "Epoch 00002: val_acc improved from 0.29576 to 0.32994, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.9432 - acc: 0.3218 - val_loss: 1.9091 - val_acc: 0.3299\n",
            "Epoch 3/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.8702 - acc: 0.3488\n",
            "Epoch 00003: val_acc improved from 0.32994 to 0.35048, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.8700 - acc: 0.3488 - val_loss: 1.8513 - val_acc: 0.3505\n",
            "Epoch 4/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.8208 - acc: 0.3667\n",
            "Epoch 00004: val_acc improved from 0.35048 to 0.36055, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.8201 - acc: 0.3666 - val_loss: 1.8372 - val_acc: 0.3605\n",
            "Epoch 5/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.7838 - acc: 0.3778\n",
            "Epoch 00005: val_acc improved from 0.36055 to 0.36176, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.7844 - acc: 0.3776 - val_loss: 1.8232 - val_acc: 0.3618\n",
            "Epoch 6/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.7522 - acc: 0.3916\n",
            "Epoch 00006: val_acc improved from 0.36176 to 0.37679, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.7531 - acc: 0.3916 - val_loss: 1.7925 - val_acc: 0.3768\n",
            "Epoch 7/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7231 - acc: 0.4022\n",
            "Epoch 00007: val_acc improved from 0.37679 to 0.39006, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.7233 - acc: 0.4021 - val_loss: 1.7635 - val_acc: 0.3901\n",
            "Epoch 8/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.7003 - acc: 0.4111\n",
            "Epoch 00008: val_acc did not improve from 0.39006\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.7000 - acc: 0.4112 - val_loss: 1.7741 - val_acc: 0.3865\n",
            "Epoch 9/100\n",
            "32768/33500 [============================>.] - ETA: 0s - loss: 1.6707 - acc: 0.4213\n",
            "Epoch 00009: val_acc improved from 0.39006 to 0.39261, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.6710 - acc: 0.4212 - val_loss: 1.7720 - val_acc: 0.3926\n",
            "Epoch 10/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.6543 - acc: 0.4285\n",
            "Epoch 00010: val_acc did not improve from 0.39261\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.6541 - acc: 0.4286 - val_loss: 1.7511 - val_acc: 0.3904\n",
            "Epoch 11/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.6318 - acc: 0.4369\n",
            "Epoch 00011: val_acc improved from 0.39261 to 0.40485, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.6317 - acc: 0.4370 - val_loss: 1.7246 - val_acc: 0.4048\n",
            "Epoch 12/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.6051 - acc: 0.4456\n",
            "Epoch 00012: val_acc did not improve from 0.40485\n",
            "33500/33500 [==============================] - 3s 87us/sample - loss: 1.6053 - acc: 0.4453 - val_loss: 1.7309 - val_acc: 0.4030\n",
            "Epoch 13/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.5882 - acc: 0.4522\n",
            "Epoch 00013: val_acc improved from 0.40485 to 0.40721, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 1.5881 - acc: 0.4522 - val_loss: 1.7143 - val_acc: 0.4072\n",
            "Epoch 14/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.5704 - acc: 0.4609\n",
            "Epoch 00014: val_acc improved from 0.40721 to 0.41182, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.5715 - acc: 0.4602 - val_loss: 1.7088 - val_acc: 0.4118\n",
            "Epoch 15/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.5524 - acc: 0.4684\n",
            "Epoch 00015: val_acc did not improve from 0.41182\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.5521 - acc: 0.4684 - val_loss: 1.7553 - val_acc: 0.3985\n",
            "Epoch 16/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.5275 - acc: 0.4751\n",
            "Epoch 00016: val_acc did not improve from 0.41182\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.5276 - acc: 0.4752 - val_loss: 1.7334 - val_acc: 0.4090\n",
            "Epoch 17/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.5088 - acc: 0.4841\n",
            "Epoch 00017: val_acc did not improve from 0.41182\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.5093 - acc: 0.4835 - val_loss: 1.7709 - val_acc: 0.4028\n",
            "Epoch 18/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.4875 - acc: 0.4929\n",
            "Epoch 00018: val_acc improved from 0.41182 to 0.42091, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 1.4872 - acc: 0.4931 - val_loss: 1.7142 - val_acc: 0.4209\n",
            "Epoch 19/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.4690 - acc: 0.4990\n",
            "Epoch 00019: val_acc did not improve from 0.42091\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.4695 - acc: 0.4988 - val_loss: 1.7518 - val_acc: 0.4115\n",
            "Epoch 20/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.4508 - acc: 0.5084\n",
            "Epoch 00020: val_acc did not improve from 0.42091\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.4506 - acc: 0.5086 - val_loss: 1.7352 - val_acc: 0.4142\n",
            "Epoch 21/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.4438 - acc: 0.5081\n",
            "Epoch 00021: val_acc did not improve from 0.42091\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.4447 - acc: 0.5076 - val_loss: 1.7450 - val_acc: 0.4195\n",
            "Epoch 22/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.4124 - acc: 0.5230\n",
            "Epoch 00022: val_acc did not improve from 0.42091\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.4130 - acc: 0.5232 - val_loss: 1.7832 - val_acc: 0.4158\n",
            "Epoch 23/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.3959 - acc: 0.5301\n",
            "Epoch 00023: val_acc improved from 0.42091 to 0.42236, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.3959 - acc: 0.5301 - val_loss: 1.7384 - val_acc: 0.4224\n",
            "Epoch 24/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.3673 - acc: 0.5379\n",
            "Epoch 00024: val_acc improved from 0.42236 to 0.42279, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.3672 - acc: 0.5380 - val_loss: 1.7481 - val_acc: 0.4228\n",
            "Epoch 25/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.3573 - acc: 0.5471\n",
            "Epoch 00025: val_acc did not improve from 0.42279\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.3571 - acc: 0.5470 - val_loss: 1.7577 - val_acc: 0.4221\n",
            "Epoch 26/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.3406 - acc: 0.5519\n",
            "Epoch 00026: val_acc did not improve from 0.42279\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 1.3408 - acc: 0.5519 - val_loss: 1.7895 - val_acc: 0.4152\n",
            "Epoch 27/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.3185 - acc: 0.5589\n",
            "Epoch 00027: val_acc did not improve from 0.42279\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.3207 - acc: 0.5579 - val_loss: 1.8072 - val_acc: 0.4155\n",
            "Epoch 28/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.3017 - acc: 0.5677\n",
            "Epoch 00028: val_acc did not improve from 0.42279\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.3022 - acc: 0.5673 - val_loss: 1.8393 - val_acc: 0.4115\n",
            "Epoch 29/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.2782 - acc: 0.5782\n",
            "Epoch 00029: val_acc did not improve from 0.42279\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.2786 - acc: 0.5782 - val_loss: 1.8049 - val_acc: 0.4213\n",
            "Epoch 30/100\n",
            "32768/33500 [============================>.] - ETA: 0s - loss: 1.2641 - acc: 0.5807\n",
            "Epoch 00030: val_acc improved from 0.42279 to 0.42612, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.2646 - acc: 0.5805 - val_loss: 1.8126 - val_acc: 0.4261\n",
            "Epoch 31/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.2412 - acc: 0.5892\n",
            "Epoch 00031: val_acc did not improve from 0.42612\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.2430 - acc: 0.5890 - val_loss: 1.8329 - val_acc: 0.4192\n",
            "Epoch 32/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.2243 - acc: 0.5992\n",
            "Epoch 00032: val_acc did not improve from 0.42612\n",
            "33500/33500 [==============================] - 3s 101us/sample - loss: 1.2241 - acc: 0.5993 - val_loss: 1.8581 - val_acc: 0.4216\n",
            "Epoch 33/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.2030 - acc: 0.6092\n",
            "Epoch 00033: val_acc did not improve from 0.42612\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.2040 - acc: 0.6088 - val_loss: 1.9111 - val_acc: 0.4150\n",
            "Epoch 34/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.1916 - acc: 0.6108\n",
            "Epoch 00034: val_acc did not improve from 0.42612\n",
            "33500/33500 [==============================] - 3s 88us/sample - loss: 1.1920 - acc: 0.6106 - val_loss: 1.9197 - val_acc: 0.4133\n",
            "Epoch 35/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.1672 - acc: 0.6230\n",
            "Epoch 00035: val_acc did not improve from 0.42612\n",
            "33500/33500 [==============================] - 3s 92us/sample - loss: 1.1673 - acc: 0.6228 - val_loss: 1.8984 - val_acc: 0.4223\n",
            "Epoch 36/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.1414 - acc: 0.6337\n",
            "Epoch 00036: val_acc did not improve from 0.42612\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.1415 - acc: 0.6337 - val_loss: 1.9668 - val_acc: 0.4094\n",
            "Epoch 37/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.1219 - acc: 0.6389\n",
            "Epoch 00037: val_acc did not improve from 0.42612\n",
            "33500/33500 [==============================] - 3s 86us/sample - loss: 1.1221 - acc: 0.6389 - val_loss: 1.9871 - val_acc: 0.4175\n",
            "Epoch 38/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.1087 - acc: 0.6481\n",
            "Epoch 00038: val_acc did not improve from 0.42612\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.1097 - acc: 0.6480 - val_loss: 1.9967 - val_acc: 0.4199\n",
            "Epoch 39/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.0913 - acc: 0.6529\n",
            "Epoch 00039: val_acc did not improve from 0.42612\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.0910 - acc: 0.6529 - val_loss: 2.0044 - val_acc: 0.4179\n",
            "Epoch 40/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.0790 - acc: 0.6583\n",
            "Epoch 00040: val_acc did not improve from 0.42612\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.0800 - acc: 0.6579 - val_loss: 2.0302 - val_acc: 0.4186\n",
            "Epoch 00040: early stopping\n",
            "10000/10000 [==============================] - 0s 38us/sample - loss: 1.7950 - acc: 0.4280\n",
            "\n",
            "Test accuracy: 42.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTVS-FlNKHcc",
        "colab_type": "text"
      },
      "source": [
        "Since it didn't reach the epoch=100, which means we still have some time, so we can decrease batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdIlD_McHUN6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78ce4f7b-bb35-4ce1-dcb1-ac82a482cc5a"
      },
      "source": [
        "# Changed batch_size\n",
        "batch_size = 64\n",
        "\n",
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of adam optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# simple early stopping\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n",
        "\n",
        "\n",
        "model.fit(x_train_gray, y_train, batch_size=batch_size, nb_epoch=100,\n",
        "      shuffle=True, verbose=1, callbacks=[es, mc], validation_split=0.33)\n",
        "\n",
        "\n",
        "saved_model = load_model('best_model.h5')\n",
        "\n",
        "loss, acc = saved_model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 396,554\n",
            "Trainable params: 396,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 33500 samples, validate on 16500 samples\n",
            "Epoch 1/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 2.1033 - acc: 0.2544\n",
            "Epoch 00001: val_acc improved from -inf to 0.27679, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 6s 180us/sample - loss: 2.1031 - acc: 0.2544 - val_loss: 2.0574 - val_acc: 0.2768\n",
            "Epoch 2/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.9408 - acc: 0.3199\n",
            "Epoch 00002: val_acc improved from 0.27679 to 0.32636, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 137us/sample - loss: 1.9405 - acc: 0.3201 - val_loss: 1.9184 - val_acc: 0.3264\n",
            "Epoch 3/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.8620 - acc: 0.3498\n",
            "Epoch 00003: val_acc improved from 0.32636 to 0.35042, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 137us/sample - loss: 1.8616 - acc: 0.3500 - val_loss: 1.8602 - val_acc: 0.3504\n",
            "Epoch 4/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.8182 - acc: 0.3640\n",
            "Epoch 00004: val_acc improved from 0.35042 to 0.35891, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.8178 - acc: 0.3642 - val_loss: 1.8242 - val_acc: 0.3589\n",
            "Epoch 5/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.7819 - acc: 0.3746\n",
            "Epoch 00005: val_acc improved from 0.35891 to 0.37339, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 148us/sample - loss: 1.7819 - acc: 0.3746 - val_loss: 1.8030 - val_acc: 0.3734\n",
            "Epoch 6/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.7507 - acc: 0.3893\n",
            "Epoch 00006: val_acc did not improve from 0.37339\n",
            "33500/33500 [==============================] - 5s 146us/sample - loss: 1.7511 - acc: 0.3890 - val_loss: 1.8027 - val_acc: 0.3720\n",
            "Epoch 7/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.7221 - acc: 0.4014\n",
            "Epoch 00007: val_acc improved from 0.37339 to 0.38103, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 137us/sample - loss: 1.7223 - acc: 0.4011 - val_loss: 1.7833 - val_acc: 0.3810\n",
            "Epoch 8/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.6983 - acc: 0.4116\n",
            "Epoch 00008: val_acc improved from 0.38103 to 0.39048, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.6973 - acc: 0.4121 - val_loss: 1.7516 - val_acc: 0.3905\n",
            "Epoch 9/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.6771 - acc: 0.4167\n",
            "Epoch 00009: val_acc did not improve from 0.39048\n",
            "33500/33500 [==============================] - 4s 134us/sample - loss: 1.6767 - acc: 0.4168 - val_loss: 1.7877 - val_acc: 0.3827\n",
            "Epoch 10/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.6574 - acc: 0.4251\n",
            "Epoch 00010: val_acc improved from 0.39048 to 0.39885, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 138us/sample - loss: 1.6571 - acc: 0.4253 - val_loss: 1.7429 - val_acc: 0.3988\n",
            "Epoch 11/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.6399 - acc: 0.4343\n",
            "Epoch 00011: val_acc did not improve from 0.39885\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.6402 - acc: 0.4343 - val_loss: 1.7738 - val_acc: 0.3875\n",
            "Epoch 12/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.6211 - acc: 0.4397\n",
            "Epoch 00012: val_acc did not improve from 0.39885\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.6209 - acc: 0.4398 - val_loss: 1.7787 - val_acc: 0.3821\n",
            "Epoch 13/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.6022 - acc: 0.4496\n",
            "Epoch 00013: val_acc improved from 0.39885 to 0.40212, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 135us/sample - loss: 1.6020 - acc: 0.4496 - val_loss: 1.7366 - val_acc: 0.4021\n",
            "Epoch 14/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.5800 - acc: 0.4586\n",
            "Epoch 00014: val_acc improved from 0.40212 to 0.40400, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.5800 - acc: 0.4587 - val_loss: 1.7390 - val_acc: 0.4040\n",
            "Epoch 15/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.5572 - acc: 0.4676\n",
            "Epoch 00015: val_acc did not improve from 0.40400\n",
            "33500/33500 [==============================] - 5s 142us/sample - loss: 1.5573 - acc: 0.4678 - val_loss: 1.7460 - val_acc: 0.4010\n",
            "Epoch 16/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.5449 - acc: 0.4725\n",
            "Epoch 00016: val_acc improved from 0.40400 to 0.40867, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.5447 - acc: 0.4726 - val_loss: 1.7339 - val_acc: 0.4087\n",
            "Epoch 17/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.5245 - acc: 0.4802\n",
            "Epoch 00017: val_acc improved from 0.40867 to 0.40982, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 4s 131us/sample - loss: 1.5245 - acc: 0.4802 - val_loss: 1.7564 - val_acc: 0.4098\n",
            "Epoch 18/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.5012 - acc: 0.4925\n",
            "Epoch 00018: val_acc improved from 0.40982 to 0.41103, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 4s 132us/sample - loss: 1.5014 - acc: 0.4924 - val_loss: 1.7582 - val_acc: 0.4110\n",
            "Epoch 19/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.4805 - acc: 0.4984\n",
            "Epoch 00019: val_acc improved from 0.41103 to 0.41412, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.4807 - acc: 0.4984 - val_loss: 1.7436 - val_acc: 0.4141\n",
            "Epoch 20/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.4647 - acc: 0.5029\n",
            "Epoch 00020: val_acc did not improve from 0.41412\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.4650 - acc: 0.5027 - val_loss: 1.7853 - val_acc: 0.4119\n",
            "Epoch 21/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.4533 - acc: 0.5077\n",
            "Epoch 00021: val_acc did not improve from 0.41412\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.4533 - acc: 0.5075 - val_loss: 1.7727 - val_acc: 0.4103\n",
            "Epoch 22/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.4317 - acc: 0.5198\n",
            "Epoch 00022: val_acc did not improve from 0.41412\n",
            "33500/33500 [==============================] - 4s 129us/sample - loss: 1.4318 - acc: 0.5199 - val_loss: 1.7950 - val_acc: 0.4044\n",
            "Epoch 23/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.4140 - acc: 0.5277\n",
            "Epoch 00023: val_acc did not improve from 0.41412\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.4144 - acc: 0.5276 - val_loss: 1.7951 - val_acc: 0.4133\n",
            "Epoch 24/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.3951 - acc: 0.5324\n",
            "Epoch 00024: val_acc did not improve from 0.41412\n",
            "33500/33500 [==============================] - 4s 132us/sample - loss: 1.3958 - acc: 0.5321 - val_loss: 1.8220 - val_acc: 0.4048\n",
            "Epoch 25/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.3825 - acc: 0.5419\n",
            "Epoch 00025: val_acc improved from 0.41412 to 0.41521, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 4s 131us/sample - loss: 1.3831 - acc: 0.5417 - val_loss: 1.8390 - val_acc: 0.4152\n",
            "Epoch 26/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.3661 - acc: 0.5452\n",
            "Epoch 00026: val_acc did not improve from 0.41521\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.3661 - acc: 0.5453 - val_loss: 1.8205 - val_acc: 0.4138\n",
            "Epoch 27/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.3366 - acc: 0.5576\n",
            "Epoch 00027: val_acc did not improve from 0.41521\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.3370 - acc: 0.5573 - val_loss: 1.8503 - val_acc: 0.4138\n",
            "Epoch 28/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.3180 - acc: 0.5641\n",
            "Epoch 00028: val_acc did not improve from 0.41521\n",
            "33500/33500 [==============================] - 4s 132us/sample - loss: 1.3201 - acc: 0.5633 - val_loss: 1.8539 - val_acc: 0.4107\n",
            "Epoch 29/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.3094 - acc: 0.5716\n",
            "Epoch 00029: val_acc did not improve from 0.41521\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.3101 - acc: 0.5711 - val_loss: 1.8858 - val_acc: 0.4127\n",
            "Epoch 30/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.2939 - acc: 0.5752\n",
            "Epoch 00030: val_acc did not improve from 0.41521\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.2937 - acc: 0.5753 - val_loss: 1.9149 - val_acc: 0.4140\n",
            "Epoch 31/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.2730 - acc: 0.5856\n",
            "Epoch 00031: val_acc did not improve from 0.41521\n",
            "33500/33500 [==============================] - 4s 134us/sample - loss: 1.2735 - acc: 0.5854 - val_loss: 1.9219 - val_acc: 0.4110\n",
            "Epoch 32/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.2561 - acc: 0.5929\n",
            "Epoch 00032: val_acc did not improve from 0.41521\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.2559 - acc: 0.5930 - val_loss: 1.9588 - val_acc: 0.4099\n",
            "Epoch 33/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.2374 - acc: 0.6031\n",
            "Epoch 00033: val_acc did not improve from 0.41521\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.2376 - acc: 0.6030 - val_loss: 1.9620 - val_acc: 0.4028\n",
            "Epoch 34/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.2241 - acc: 0.6044\n",
            "Epoch 00034: val_acc did not improve from 0.41521\n",
            "33500/33500 [==============================] - 5s 135us/sample - loss: 1.2231 - acc: 0.6048 - val_loss: 1.9796 - val_acc: 0.4116\n",
            "Epoch 35/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.2106 - acc: 0.6154\n",
            "Epoch 00035: val_acc did not improve from 0.41521\n",
            "33500/33500 [==============================] - 5s 135us/sample - loss: 1.2100 - acc: 0.6157 - val_loss: 2.0235 - val_acc: 0.4113\n",
            "Epoch 00035: early stopping\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 1.8142 - acc: 0.4148\n",
            "\n",
            "Test accuracy: 41.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49Jog98pKN79",
        "colab_type": "text"
      },
      "source": [
        "It overfitted too early in the epoch; which means that our regularizer is not effective anymore. Let's change it into dropout, while adding dataset augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCnokHV2LKS6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2950369-955f-46d0-9443-53c8d2613671"
      },
      "source": [
        "# Changed batch_size\n",
        "batch_size = 64\n",
        "\n",
        "# Changed into dropout\n",
        "dropout = 0.2\n",
        "\n",
        "# this is 3-layer MLP with ReLU. \n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "# simple early stopping\n",
        "# mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "# es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the network\n",
        "data_augmentation = True\n",
        "epochs = 50\n",
        "max_batches = len(x_train) / batch_size\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    # train the network no data augmentation\n",
        "    x_train_gray = np.reshape(x_train_gray, [-1, input_size])\n",
        "    model.fit(x_train_gray, y_train, epochs=epochs, batch_size=batch_size)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    # we need [width, height, channel] dim for data aug\n",
        "    x_train_gray = np.reshape(x_train_gray, [-1, image_size, image_size, 1])\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=5.0,  # randomly rotate images in the range (deg 0 to 180)\n",
        "        width_shift_range=2.0,  # randomly shift images horizontally\n",
        "        height_shift_range=2.0,  # randomly shift images vertically\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True)  # randomly flip images\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train_gray)\n",
        "    for e in range(epochs):\n",
        "        batches = 0\n",
        "        for x_batch, y_batch in datagen.flow(x_train_gray, y_train, batch_size=batch_size):\n",
        "            x_batch = np.reshape(x_batch, [-1, input_size])\n",
        "            model.fit(x_batch, y_batch, verbose=0)\n",
        "            batches += 1\n",
        "            \n",
        "            if batches >= max_batches:\n",
        "                # we need to break the loop by hand because\n",
        "                # the generator loops indefinitely\n",
        "                break\n",
        "        print(\"Epoch %d/%d\" % (e+1, epochs))\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "score = model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_29 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 396,554\n",
            "Trainable params: 396,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "Epoch 2/50\n",
            "Epoch 3/50\n",
            "Epoch 4/50\n",
            "Epoch 5/50\n",
            "Epoch 6/50\n",
            "Epoch 7/50\n",
            "Epoch 8/50\n",
            "Epoch 9/50\n",
            "Epoch 10/50\n",
            "Epoch 11/50\n",
            "Epoch 12/50\n",
            "Epoch 13/50\n",
            "Epoch 14/50\n",
            "Epoch 15/50\n",
            "Epoch 16/50\n",
            "Epoch 17/50\n",
            "Epoch 18/50\n",
            "Epoch 19/50\n",
            "Epoch 20/50\n",
            "Epoch 21/50\n",
            "Epoch 22/50\n",
            "Epoch 23/50\n",
            "Epoch 24/50\n",
            "Epoch 25/50\n",
            "Epoch 26/50\n",
            "Epoch 27/50\n",
            "Epoch 28/50\n",
            "Epoch 29/50\n",
            "Epoch 30/50\n",
            "Epoch 31/50\n",
            "Epoch 32/50\n",
            "Epoch 33/50\n",
            "Epoch 34/50\n",
            "Epoch 35/50\n",
            "Epoch 36/50\n",
            "Epoch 37/50\n",
            "Epoch 38/50\n",
            "Epoch 39/50\n",
            "Epoch 40/50\n",
            "Epoch 41/50\n",
            "Epoch 42/50\n",
            "Epoch 43/50\n",
            "Epoch 44/50\n",
            "Epoch 45/50\n",
            "Epoch 46/50\n",
            "Epoch 47/50\n",
            "Epoch 48/50\n",
            "Epoch 49/50\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 64us/sample - loss: 2.0430 - acc: 0.2682\n",
            "\n",
            "Test accuracy: 26.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOhtOTHxL5tN",
        "colab_type": "text"
      },
      "source": [
        "Dropout + Dataset Augmentation didn't work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myi7TyKEP8CV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aceb9416-9074-446e-a203-768ed5b64abc"
      },
      "source": [
        "# Changed batch_size\n",
        "batch_size = 64\n",
        "\n",
        "# Regularizer\n",
        "dropout = 0.2\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of adam optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# simple early stopping\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n",
        "\n",
        "\n",
        "model.fit(x_train_gray, y_train, batch_size=batch_size, nb_epoch=100,\n",
        "      shuffle=True, verbose=1, callbacks=[es, mc], validation_split=0.33)\n",
        "\n",
        "\n",
        "saved_model = load_model('best_model.h5')\n",
        "\n",
        "loss, acc = saved_model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_37 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 396,554\n",
            "Trainable params: 396,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 33500 samples, validate on 16500 samples\n",
            "Epoch 1/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 2.1292 - acc: 0.2065\n",
            "Epoch 00001: val_acc improved from -inf to 0.26327, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 198us/sample - loss: 2.1286 - acc: 0.2069 - val_loss: 2.0184 - val_acc: 0.2633\n",
            "Epoch 2/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 2.0248 - acc: 0.2615\n",
            "Epoch 00002: val_acc improved from 0.26327 to 0.30224, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 144us/sample - loss: 2.0247 - acc: 0.2615 - val_loss: 1.9474 - val_acc: 0.3022\n",
            "Epoch 3/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.9807 - acc: 0.2805\n",
            "Epoch 00003: val_acc did not improve from 0.30224\n",
            "33500/33500 [==============================] - 5s 146us/sample - loss: 1.9807 - acc: 0.2805 - val_loss: 1.9385 - val_acc: 0.2962\n",
            "Epoch 4/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.9502 - acc: 0.2929\n",
            "Epoch 00004: val_acc improved from 0.30224 to 0.30988, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 142us/sample - loss: 1.9496 - acc: 0.2933 - val_loss: 1.9107 - val_acc: 0.3099\n",
            "Epoch 5/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.9313 - acc: 0.3004\n",
            "Epoch 00005: val_acc improved from 0.30988 to 0.32394, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.9314 - acc: 0.3005 - val_loss: 1.8980 - val_acc: 0.3239\n",
            "Epoch 6/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.9265 - acc: 0.3001\n",
            "Epoch 00006: val_acc did not improve from 0.32394\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.9259 - acc: 0.3001 - val_loss: 1.9046 - val_acc: 0.3105\n",
            "Epoch 7/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.9073 - acc: 0.3082\n",
            "Epoch 00007: val_acc did not improve from 0.32394\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.9075 - acc: 0.3081 - val_loss: 1.9039 - val_acc: 0.3134\n",
            "Epoch 8/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.8926 - acc: 0.3177\n",
            "Epoch 00008: val_acc did not improve from 0.32394\n",
            "33500/33500 [==============================] - 5s 137us/sample - loss: 1.8925 - acc: 0.3178 - val_loss: 1.8693 - val_acc: 0.3231\n",
            "Epoch 9/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.8854 - acc: 0.3198\n",
            "Epoch 00009: val_acc did not improve from 0.32394\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.8858 - acc: 0.3196 - val_loss: 1.9021 - val_acc: 0.3202\n",
            "Epoch 10/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.8797 - acc: 0.3220\n",
            "Epoch 00010: val_acc improved from 0.32394 to 0.32558, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 145us/sample - loss: 1.8799 - acc: 0.3218 - val_loss: 1.8891 - val_acc: 0.3256\n",
            "Epoch 11/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.8682 - acc: 0.3267\n",
            "Epoch 00011: val_acc improved from 0.32558 to 0.34655, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.8680 - acc: 0.3267 - val_loss: 1.8243 - val_acc: 0.3465\n",
            "Epoch 12/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.8603 - acc: 0.3309\n",
            "Epoch 00012: val_acc did not improve from 0.34655\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.8612 - acc: 0.3300 - val_loss: 1.8652 - val_acc: 0.3331\n",
            "Epoch 13/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.8547 - acc: 0.3305\n",
            "Epoch 00013: val_acc improved from 0.34655 to 0.34788, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 145us/sample - loss: 1.8544 - acc: 0.3306 - val_loss: 1.8193 - val_acc: 0.3479\n",
            "Epoch 14/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.8528 - acc: 0.3310\n",
            "Epoch 00014: val_acc did not improve from 0.34788\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.8529 - acc: 0.3311 - val_loss: 1.8504 - val_acc: 0.3347\n",
            "Epoch 15/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.8446 - acc: 0.3351\n",
            "Epoch 00015: val_acc did not improve from 0.34788\n",
            "33500/33500 [==============================] - 5s 146us/sample - loss: 1.8442 - acc: 0.3353 - val_loss: 1.8194 - val_acc: 0.3468\n",
            "Epoch 16/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.8378 - acc: 0.3385\n",
            "Epoch 00016: val_acc improved from 0.34788 to 0.35382, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.8381 - acc: 0.3383 - val_loss: 1.8122 - val_acc: 0.3538\n",
            "Epoch 17/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.8344 - acc: 0.3395\n",
            "Epoch 00017: val_acc did not improve from 0.35382\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.8342 - acc: 0.3396 - val_loss: 1.8354 - val_acc: 0.3459\n",
            "Epoch 18/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.8368 - acc: 0.3409\n",
            "Epoch 00018: val_acc did not improve from 0.35382\n",
            "33500/33500 [==============================] - 5s 147us/sample - loss: 1.8365 - acc: 0.3413 - val_loss: 1.8150 - val_acc: 0.3516\n",
            "Epoch 19/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.8343 - acc: 0.3406\n",
            "Epoch 00019: val_acc did not improve from 0.35382\n",
            "33500/33500 [==============================] - 5s 147us/sample - loss: 1.8342 - acc: 0.3406 - val_loss: 1.8315 - val_acc: 0.3364\n",
            "Epoch 20/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.8295 - acc: 0.3438\n",
            "Epoch 00020: val_acc did not improve from 0.35382\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.8296 - acc: 0.3439 - val_loss: 1.8180 - val_acc: 0.3501\n",
            "Epoch 21/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.8185 - acc: 0.3461\n",
            "Epoch 00021: val_acc did not improve from 0.35382\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.8186 - acc: 0.3460 - val_loss: 1.8186 - val_acc: 0.3500\n",
            "Epoch 22/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.8137 - acc: 0.3497\n",
            "Epoch 00022: val_acc improved from 0.35382 to 0.36085, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.8137 - acc: 0.3497 - val_loss: 1.8006 - val_acc: 0.3608\n",
            "Epoch 23/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.8168 - acc: 0.3485\n",
            "Epoch 00023: val_acc did not improve from 0.36085\n",
            "33500/33500 [==============================] - 5s 150us/sample - loss: 1.8166 - acc: 0.3486 - val_loss: 1.8251 - val_acc: 0.3465\n",
            "Epoch 24/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.8099 - acc: 0.3507\n",
            "Epoch 00024: val_acc did not improve from 0.36085\n",
            "33500/33500 [==============================] - 5s 148us/sample - loss: 1.8095 - acc: 0.3510 - val_loss: 1.7934 - val_acc: 0.3591\n",
            "Epoch 25/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.8062 - acc: 0.3500\n",
            "Epoch 00025: val_acc improved from 0.36085 to 0.36176, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 147us/sample - loss: 1.8064 - acc: 0.3498 - val_loss: 1.7881 - val_acc: 0.3618\n",
            "Epoch 26/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.8022 - acc: 0.3491\n",
            "Epoch 00026: val_acc improved from 0.36176 to 0.36230, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 148us/sample - loss: 1.8012 - acc: 0.3493 - val_loss: 1.7765 - val_acc: 0.3623\n",
            "Epoch 27/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.7939 - acc: 0.3547\n",
            "Epoch 00027: val_acc did not improve from 0.36230\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.7938 - acc: 0.3549 - val_loss: 1.7951 - val_acc: 0.3605\n",
            "Epoch 28/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.7976 - acc: 0.3550\n",
            "Epoch 00028: val_acc did not improve from 0.36230\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.7977 - acc: 0.3550 - val_loss: 1.7966 - val_acc: 0.3555\n",
            "Epoch 29/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7963 - acc: 0.3543\n",
            "Epoch 00029: val_acc did not improve from 0.36230\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.7964 - acc: 0.3544 - val_loss: 1.8071 - val_acc: 0.3619\n",
            "Epoch 30/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7873 - acc: 0.3581\n",
            "Epoch 00030: val_acc improved from 0.36230 to 0.36339, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.7872 - acc: 0.3580 - val_loss: 1.7848 - val_acc: 0.3634\n",
            "Epoch 31/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.7875 - acc: 0.3587\n",
            "Epoch 00031: val_acc did not improve from 0.36339\n",
            "33500/33500 [==============================] - 5s 138us/sample - loss: 1.7875 - acc: 0.3586 - val_loss: 1.7994 - val_acc: 0.3582\n",
            "Epoch 32/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.7801 - acc: 0.3638\n",
            "Epoch 00032: val_acc did not improve from 0.36339\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.7792 - acc: 0.3642 - val_loss: 1.7922 - val_acc: 0.3572\n",
            "Epoch 33/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.7810 - acc: 0.3606\n",
            "Epoch 00033: val_acc did not improve from 0.36339\n",
            "33500/33500 [==============================] - 5s 145us/sample - loss: 1.7813 - acc: 0.3604 - val_loss: 1.7949 - val_acc: 0.3608\n",
            "Epoch 34/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.7804 - acc: 0.3611\n",
            "Epoch 00034: val_acc did not improve from 0.36339\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.7798 - acc: 0.3612 - val_loss: 1.7961 - val_acc: 0.3599\n",
            "Epoch 35/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7751 - acc: 0.3628\n",
            "Epoch 00035: val_acc did not improve from 0.36339\n",
            "33500/33500 [==============================] - 5s 139us/sample - loss: 1.7746 - acc: 0.3628 - val_loss: 1.7926 - val_acc: 0.3587\n",
            "Epoch 36/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7756 - acc: 0.3632\n",
            "Epoch 00036: val_acc improved from 0.36339 to 0.36709, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.7759 - acc: 0.3630 - val_loss: 1.7742 - val_acc: 0.3671\n",
            "Epoch 37/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.7708 - acc: 0.3634\n",
            "Epoch 00037: val_acc did not improve from 0.36709\n",
            "33500/33500 [==============================] - 5s 147us/sample - loss: 1.7707 - acc: 0.3635 - val_loss: 1.7871 - val_acc: 0.3595\n",
            "Epoch 38/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7710 - acc: 0.3624\n",
            "Epoch 00038: val_acc improved from 0.36709 to 0.37400, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.7711 - acc: 0.3624 - val_loss: 1.7622 - val_acc: 0.3740\n",
            "Epoch 39/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.7689 - acc: 0.3640\n",
            "Epoch 00039: val_acc did not improve from 0.37400\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.7689 - acc: 0.3641 - val_loss: 1.7707 - val_acc: 0.3675\n",
            "Epoch 40/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.7598 - acc: 0.3666\n",
            "Epoch 00040: val_acc did not improve from 0.37400\n",
            "33500/33500 [==============================] - 5s 136us/sample - loss: 1.7596 - acc: 0.3666 - val_loss: 1.7925 - val_acc: 0.3670\n",
            "Epoch 41/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7629 - acc: 0.3667\n",
            "Epoch 00041: val_acc did not improve from 0.37400\n",
            "33500/33500 [==============================] - 5s 137us/sample - loss: 1.7631 - acc: 0.3665 - val_loss: 1.7828 - val_acc: 0.3628\n",
            "Epoch 42/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.7660 - acc: 0.3667\n",
            "Epoch 00042: val_acc did not improve from 0.37400\n",
            "33500/33500 [==============================] - 5s 138us/sample - loss: 1.7659 - acc: 0.3665 - val_loss: 1.7716 - val_acc: 0.3641\n",
            "Epoch 43/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.7573 - acc: 0.3726\n",
            "Epoch 00043: val_acc did not improve from 0.37400\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.7582 - acc: 0.3724 - val_loss: 1.8150 - val_acc: 0.3565\n",
            "Epoch 44/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7583 - acc: 0.3687\n",
            "Epoch 00044: val_acc did not improve from 0.37400\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.7582 - acc: 0.3688 - val_loss: 1.8230 - val_acc: 0.3427\n",
            "Epoch 45/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7558 - acc: 0.3692\n",
            "Epoch 00045: val_acc did not improve from 0.37400\n",
            "33500/33500 [==============================] - 4s 132us/sample - loss: 1.7557 - acc: 0.3689 - val_loss: 1.7883 - val_acc: 0.3658\n",
            "Epoch 46/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.7563 - acc: 0.3701\n",
            "Epoch 00046: val_acc did not improve from 0.37400\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.7563 - acc: 0.3699 - val_loss: 1.7549 - val_acc: 0.3731\n",
            "Epoch 47/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.7468 - acc: 0.3735\n",
            "Epoch 00047: val_acc improved from 0.37400 to 0.37697, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 144us/sample - loss: 1.7469 - acc: 0.3735 - val_loss: 1.7648 - val_acc: 0.3770\n",
            "Epoch 48/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.7543 - acc: 0.3721\n",
            "Epoch 00048: val_acc did not improve from 0.37697\n",
            "33500/33500 [==============================] - 5s 144us/sample - loss: 1.7535 - acc: 0.3723 - val_loss: 1.7738 - val_acc: 0.3627\n",
            "Epoch 49/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7440 - acc: 0.3736\n",
            "Epoch 00049: val_acc did not improve from 0.37697\n",
            "33500/33500 [==============================] - 5s 142us/sample - loss: 1.7436 - acc: 0.3738 - val_loss: 1.7812 - val_acc: 0.3710\n",
            "Epoch 50/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7399 - acc: 0.3742\n",
            "Epoch 00050: val_acc did not improve from 0.37697\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.7400 - acc: 0.3740 - val_loss: 1.7678 - val_acc: 0.3669\n",
            "Epoch 51/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7434 - acc: 0.3721\n",
            "Epoch 00051: val_acc did not improve from 0.37697\n",
            "33500/33500 [==============================] - 5s 138us/sample - loss: 1.7431 - acc: 0.3723 - val_loss: 1.7892 - val_acc: 0.3659\n",
            "Epoch 52/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.7444 - acc: 0.3753\n",
            "Epoch 00052: val_acc did not improve from 0.37697\n",
            "33500/33500 [==============================] - 5s 147us/sample - loss: 1.7450 - acc: 0.3750 - val_loss: 1.7647 - val_acc: 0.3692\n",
            "Epoch 53/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.7388 - acc: 0.3693\n",
            "Epoch 00053: val_acc improved from 0.37697 to 0.38109, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.7392 - acc: 0.3696 - val_loss: 1.7360 - val_acc: 0.3811\n",
            "Epoch 54/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.7347 - acc: 0.3782\n",
            "Epoch 00054: val_acc did not improve from 0.38109\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.7342 - acc: 0.3785 - val_loss: 1.7955 - val_acc: 0.3605\n",
            "Epoch 55/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.7313 - acc: 0.3778\n",
            "Epoch 00055: val_acc did not improve from 0.38109\n",
            "33500/33500 [==============================] - 5s 135us/sample - loss: 1.7320 - acc: 0.3776 - val_loss: 1.7678 - val_acc: 0.3702\n",
            "Epoch 56/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.7325 - acc: 0.3799\n",
            "Epoch 00056: val_acc did not improve from 0.38109\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.7319 - acc: 0.3802 - val_loss: 1.7871 - val_acc: 0.3653\n",
            "Epoch 57/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7377 - acc: 0.3768\n",
            "Epoch 00057: val_acc did not improve from 0.38109\n",
            "33500/33500 [==============================] - 5s 143us/sample - loss: 1.7381 - acc: 0.3767 - val_loss: 1.7621 - val_acc: 0.3799\n",
            "Epoch 58/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.7382 - acc: 0.3759\n",
            "Epoch 00058: val_acc did not improve from 0.38109\n",
            "33500/33500 [==============================] - 5s 136us/sample - loss: 1.7390 - acc: 0.3759 - val_loss: 1.7512 - val_acc: 0.3761\n",
            "Epoch 59/100\n",
            "33088/33500 [============================>.] - ETA: 0s - loss: 1.7300 - acc: 0.3760\n",
            "Epoch 00059: val_acc did not improve from 0.38109\n",
            "33500/33500 [==============================] - 5s 140us/sample - loss: 1.7311 - acc: 0.3755 - val_loss: 1.7642 - val_acc: 0.3710\n",
            "Epoch 60/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.7347 - acc: 0.3759\n",
            "Epoch 00060: val_acc did not improve from 0.38109\n",
            "33500/33500 [==============================] - 5s 137us/sample - loss: 1.7350 - acc: 0.3758 - val_loss: 1.7683 - val_acc: 0.3692\n",
            "Epoch 61/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.7291 - acc: 0.3792\n",
            "Epoch 00061: val_acc did not improve from 0.38109\n",
            "33500/33500 [==============================] - 4s 134us/sample - loss: 1.7288 - acc: 0.3794 - val_loss: 1.7533 - val_acc: 0.3759\n",
            "Epoch 62/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7265 - acc: 0.3800\n",
            "Epoch 00062: val_acc did not improve from 0.38109\n",
            "33500/33500 [==============================] - 5s 137us/sample - loss: 1.7264 - acc: 0.3802 - val_loss: 1.7728 - val_acc: 0.3712\n",
            "Epoch 63/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7281 - acc: 0.3788\n",
            "Epoch 00063: val_acc did not improve from 0.38109\n",
            "33500/33500 [==============================] - 5s 141us/sample - loss: 1.7284 - acc: 0.3787 - val_loss: 1.8026 - val_acc: 0.3624\n",
            "Epoch 00063: early stopping\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 1.7260 - acc: 0.3841\n",
            "\n",
            "Test accuracy: 38.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1DnsFRuRje0",
        "colab_type": "text"
      },
      "source": [
        "It didn't improve too. Let's just try decreasing the batch size further"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiE-jKXfQEaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "faa23709-9e8c-43a0-d210-206da594aca8"
      },
      "source": [
        "# Changed batch_size v2\n",
        "batch_size = 32\n",
        "\n",
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of adam optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# simple early stopping\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n",
        "\n",
        "\n",
        "model.fit(x_train_gray, y_train, batch_size=batch_size, nb_epoch=100,\n",
        "      shuffle=True, verbose=1, callbacks=[es, mc], validation_split=0.33)\n",
        "\n",
        "\n",
        "saved_model = load_model('best_model.h5')\n",
        "\n",
        "loss, acc = saved_model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_41 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 396,554\n",
            "Trainable params: 396,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 33500 samples, validate on 16500 samples\n",
            "Epoch 1/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 2.0876 - acc: 0.2595\n",
            "Epoch 00001: val_acc improved from -inf to 0.29303, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 9s 258us/sample - loss: 2.0876 - acc: 0.2595 - val_loss: 1.9921 - val_acc: 0.2930\n",
            "Epoch 2/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.9395 - acc: 0.3165\n",
            "Epoch 00002: val_acc improved from 0.29303 to 0.31691, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 8s 233us/sample - loss: 1.9397 - acc: 0.3165 - val_loss: 1.9233 - val_acc: 0.3169\n",
            "Epoch 3/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.8707 - acc: 0.3418\n",
            "Epoch 00003: val_acc improved from 0.31691 to 0.32945, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 220us/sample - loss: 1.8708 - acc: 0.3414 - val_loss: 1.9037 - val_acc: 0.3295\n",
            "Epoch 4/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.8257 - acc: 0.3563\n",
            "Epoch 00004: val_acc improved from 0.32945 to 0.35485, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 216us/sample - loss: 1.8257 - acc: 0.3565 - val_loss: 1.8450 - val_acc: 0.3548\n",
            "Epoch 5/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.7962 - acc: 0.3691\n",
            "Epoch 00005: val_acc did not improve from 0.35485\n",
            "33500/33500 [==============================] - 8s 226us/sample - loss: 1.7962 - acc: 0.3691 - val_loss: 1.8420 - val_acc: 0.3517\n",
            "Epoch 6/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.7720 - acc: 0.3776\n",
            "Epoch 00006: val_acc improved from 0.35485 to 0.36788, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 223us/sample - loss: 1.7719 - acc: 0.3778 - val_loss: 1.8103 - val_acc: 0.3679\n",
            "Epoch 7/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.7478 - acc: 0.3870\n",
            "Epoch 00007: val_acc improved from 0.36788 to 0.37424, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 205us/sample - loss: 1.7484 - acc: 0.3871 - val_loss: 1.7978 - val_acc: 0.3742\n",
            "Epoch 8/100\n",
            "33440/33500 [============================>.] - ETA: 0s - loss: 1.7314 - acc: 0.3954\n",
            "Epoch 00008: val_acc did not improve from 0.37424\n",
            "33500/33500 [==============================] - 7s 205us/sample - loss: 1.7315 - acc: 0.3953 - val_loss: 1.8193 - val_acc: 0.3668\n",
            "Epoch 9/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.7101 - acc: 0.4056\n",
            "Epoch 00009: val_acc did not improve from 0.37424\n",
            "33500/33500 [==============================] - 7s 215us/sample - loss: 1.7107 - acc: 0.4051 - val_loss: 1.8536 - val_acc: 0.3606\n",
            "Epoch 10/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.6972 - acc: 0.4089\n",
            "Epoch 00010: val_acc improved from 0.37424 to 0.39115, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 219us/sample - loss: 1.6968 - acc: 0.4092 - val_loss: 1.7667 - val_acc: 0.3912\n",
            "Epoch 11/100\n",
            "33440/33500 [============================>.] - ETA: 0s - loss: 1.6768 - acc: 0.4171\n",
            "Epoch 00011: val_acc improved from 0.39115 to 0.39824, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 207us/sample - loss: 1.6770 - acc: 0.4170 - val_loss: 1.7537 - val_acc: 0.3982\n",
            "Epoch 12/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.6623 - acc: 0.4242\n",
            "Epoch 00012: val_acc did not improve from 0.39824\n",
            "33500/33500 [==============================] - 7s 206us/sample - loss: 1.6623 - acc: 0.4241 - val_loss: 1.7671 - val_acc: 0.3872\n",
            "Epoch 13/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.6520 - acc: 0.4309\n",
            "Epoch 00013: val_acc did not improve from 0.39824\n",
            "33500/33500 [==============================] - 7s 206us/sample - loss: 1.6524 - acc: 0.4306 - val_loss: 1.7704 - val_acc: 0.3923\n",
            "Epoch 14/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.6359 - acc: 0.4332\n",
            "Epoch 00014: val_acc did not improve from 0.39824\n",
            "33500/33500 [==============================] - 7s 206us/sample - loss: 1.6362 - acc: 0.4332 - val_loss: 1.8016 - val_acc: 0.3822\n",
            "Epoch 15/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.6231 - acc: 0.4415\n",
            "Epoch 00015: val_acc improved from 0.39824 to 0.40267, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 207us/sample - loss: 1.6225 - acc: 0.4416 - val_loss: 1.7587 - val_acc: 0.4027\n",
            "Epoch 16/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.6083 - acc: 0.4496\n",
            "Epoch 00016: val_acc did not improve from 0.40267\n",
            "33500/33500 [==============================] - 7s 202us/sample - loss: 1.6087 - acc: 0.4496 - val_loss: 1.7821 - val_acc: 0.3917\n",
            "Epoch 17/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.5976 - acc: 0.4510\n",
            "Epoch 00017: val_acc did not improve from 0.40267\n",
            "33500/33500 [==============================] - 7s 201us/sample - loss: 1.5976 - acc: 0.4511 - val_loss: 1.8237 - val_acc: 0.3812\n",
            "Epoch 18/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.5853 - acc: 0.4581\n",
            "Epoch 00018: val_acc did not improve from 0.40267\n",
            "33500/33500 [==============================] - 7s 208us/sample - loss: 1.5856 - acc: 0.4580 - val_loss: 1.7973 - val_acc: 0.3944\n",
            "Epoch 19/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.5707 - acc: 0.4584\n",
            "Epoch 00019: val_acc did not improve from 0.40267\n",
            "33500/33500 [==============================] - 7s 209us/sample - loss: 1.5712 - acc: 0.4582 - val_loss: 1.7989 - val_acc: 0.3981\n",
            "Epoch 20/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.5632 - acc: 0.4638\n",
            "Epoch 00020: val_acc did not improve from 0.40267\n",
            "33500/33500 [==============================] - 7s 205us/sample - loss: 1.5635 - acc: 0.4639 - val_loss: 1.7992 - val_acc: 0.3964\n",
            "Epoch 21/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.5434 - acc: 0.4731\n",
            "Epoch 00021: val_acc did not improve from 0.40267\n",
            "33500/33500 [==============================] - 7s 206us/sample - loss: 1.5441 - acc: 0.4730 - val_loss: 1.8287 - val_acc: 0.3835\n",
            "Epoch 22/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.5350 - acc: 0.4783\n",
            "Epoch 00022: val_acc did not improve from 0.40267\n",
            "33500/33500 [==============================] - 7s 210us/sample - loss: 1.5359 - acc: 0.4779 - val_loss: 1.8153 - val_acc: 0.3900\n",
            "Epoch 23/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.5210 - acc: 0.4854\n",
            "Epoch 00023: val_acc did not improve from 0.40267\n",
            "33500/33500 [==============================] - 7s 195us/sample - loss: 1.5211 - acc: 0.4853 - val_loss: 1.8118 - val_acc: 0.3967\n",
            "Epoch 24/100\n",
            "33312/33500 [============================>.] - ETA: 0s - loss: 1.5112 - acc: 0.4878\n",
            "Epoch 00024: val_acc improved from 0.40267 to 0.40394, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 211us/sample - loss: 1.5111 - acc: 0.4877 - val_loss: 1.8206 - val_acc: 0.4039\n",
            "Epoch 25/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.4935 - acc: 0.4936\n",
            "Epoch 00025: val_acc did not improve from 0.40394\n",
            "33500/33500 [==============================] - 7s 211us/sample - loss: 1.4935 - acc: 0.4936 - val_loss: 1.8757 - val_acc: 0.3890\n",
            "Epoch 26/100\n",
            "33248/33500 [============================>.] - ETA: 0s - loss: 1.4862 - acc: 0.4983\n",
            "Epoch 00026: val_acc did not improve from 0.40394\n",
            "33500/33500 [==============================] - 7s 200us/sample - loss: 1.4868 - acc: 0.4981 - val_loss: 1.8378 - val_acc: 0.4008\n",
            "Epoch 27/100\n",
            "33440/33500 [============================>.] - ETA: 0s - loss: 1.4738 - acc: 0.5002\n",
            "Epoch 00027: val_acc did not improve from 0.40394\n",
            "33500/33500 [==============================] - 7s 195us/sample - loss: 1.4739 - acc: 0.5003 - val_loss: 1.8631 - val_acc: 0.3966\n",
            "Epoch 28/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.4603 - acc: 0.5081\n",
            "Epoch 00028: val_acc did not improve from 0.40394\n",
            "33500/33500 [==============================] - 7s 208us/sample - loss: 1.4608 - acc: 0.5080 - val_loss: 1.8912 - val_acc: 0.3884\n",
            "Epoch 29/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.4503 - acc: 0.5140\n",
            "Epoch 00029: val_acc did not improve from 0.40394\n",
            "33500/33500 [==============================] - 7s 211us/sample - loss: 1.4501 - acc: 0.5139 - val_loss: 1.8950 - val_acc: 0.3865\n",
            "Epoch 30/100\n",
            "33184/33500 [============================>.] - ETA: 0s - loss: 1.4309 - acc: 0.5222\n",
            "Epoch 00030: val_acc did not improve from 0.40394\n",
            "33500/33500 [==============================] - 7s 208us/sample - loss: 1.4315 - acc: 0.5220 - val_loss: 1.9021 - val_acc: 0.3918\n",
            "Epoch 31/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.4252 - acc: 0.5277\n",
            "Epoch 00031: val_acc did not improve from 0.40394\n",
            "33500/33500 [==============================] - 7s 206us/sample - loss: 1.4250 - acc: 0.5277 - val_loss: 1.8979 - val_acc: 0.3964\n",
            "Epoch 32/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.4098 - acc: 0.5307\n",
            "Epoch 00032: val_acc did not improve from 0.40394\n",
            "33500/33500 [==============================] - 7s 215us/sample - loss: 1.4095 - acc: 0.5307 - val_loss: 1.9396 - val_acc: 0.3890\n",
            "Epoch 33/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.4024 - acc: 0.5338\n",
            "Epoch 00033: val_acc did not improve from 0.40394\n",
            "33500/33500 [==============================] - 7s 199us/sample - loss: 1.4025 - acc: 0.5337 - val_loss: 1.9657 - val_acc: 0.3872\n",
            "Epoch 34/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.3886 - acc: 0.5422\n",
            "Epoch 00034: val_acc did not improve from 0.40394\n",
            "33500/33500 [==============================] - 7s 216us/sample - loss: 1.3890 - acc: 0.5421 - val_loss: 1.9839 - val_acc: 0.3882\n",
            "Epoch 00034: early stopping\n",
            "10000/10000 [==============================] - 1s 84us/sample - loss: 1.7964 - acc: 0.4075\n",
            "\n",
            "Test accuracy: 40.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58PlW0WpRrin",
        "colab_type": "text"
      },
      "source": [
        "It didn't improve again. Let's try increasing the penalty so that it won't overfit @ early stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKSGUG4dSyfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f801208a-572a-4fd2-a978-aafb97120525"
      },
      "source": [
        "# Changed batch_size v2\n",
        "batch_size = 128\n",
        "\n",
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of adam optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# simple early stopping\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n",
        "\n",
        "\n",
        "model.fit(x_train_gray, y_train, batch_size=batch_size, nb_epoch=100,\n",
        "      shuffle=True, verbose=1, callbacks=[es, mc], validation_split=0.33)\n",
        "\n",
        "\n",
        "saved_model = load_model('best_model.h5')\n",
        "\n",
        "loss, acc = saved_model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_45 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 396,554\n",
            "Trainable params: 396,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 33500 samples, validate on 16500 samples\n",
            "Epoch 1/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 2.2690 - acc: 0.2485\n",
            "Epoch 00001: val_acc improved from -inf to 0.28145, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 9s 274us/sample - loss: 2.2675 - acc: 0.2487 - val_loss: 2.0737 - val_acc: 0.2815\n",
            "Epoch 2/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 2.0244 - acc: 0.2978\n",
            "Epoch 00002: val_acc did not improve from 0.28145\n",
            "33500/33500 [==============================] - 7s 205us/sample - loss: 2.0245 - acc: 0.2977 - val_loss: 2.0992 - val_acc: 0.2716\n",
            "Epoch 3/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.9700 - acc: 0.3166\n",
            "Epoch 00003: val_acc improved from 0.28145 to 0.31358, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 202us/sample - loss: 1.9701 - acc: 0.3165 - val_loss: 1.9757 - val_acc: 0.3136\n",
            "Epoch 4/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.9304 - acc: 0.3302\n",
            "Epoch 00004: val_acc improved from 0.31358 to 0.33788, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 216us/sample - loss: 1.9304 - acc: 0.3302 - val_loss: 1.9243 - val_acc: 0.3379\n",
            "Epoch 5/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.9024 - acc: 0.3411\n",
            "Epoch 00005: val_acc did not improve from 0.33788\n",
            "33500/33500 [==============================] - 7s 210us/sample - loss: 1.9026 - acc: 0.3410 - val_loss: 1.9169 - val_acc: 0.3336\n",
            "Epoch 6/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.8868 - acc: 0.3499\n",
            "Epoch 00006: val_acc improved from 0.33788 to 0.34582, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 218us/sample - loss: 1.8869 - acc: 0.3497 - val_loss: 1.8955 - val_acc: 0.3458\n",
            "Epoch 7/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.8735 - acc: 0.3544\n",
            "Epoch 00007: val_acc improved from 0.34582 to 0.34879, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 218us/sample - loss: 1.8738 - acc: 0.3542 - val_loss: 1.8894 - val_acc: 0.3488\n",
            "Epoch 8/100\n",
            "33440/33500 [============================>.] - ETA: 0s - loss: 1.8599 - acc: 0.3609\n",
            "Epoch 00008: val_acc improved from 0.34879 to 0.36679, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 203us/sample - loss: 1.8598 - acc: 0.3610 - val_loss: 1.8481 - val_acc: 0.3668\n",
            "Epoch 9/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.8439 - acc: 0.3672\n",
            "Epoch 00009: val_acc did not improve from 0.36679\n",
            "33500/33500 [==============================] - 7s 203us/sample - loss: 1.8441 - acc: 0.3672 - val_loss: 1.8592 - val_acc: 0.3640\n",
            "Epoch 10/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.8385 - acc: 0.3731\n",
            "Epoch 00010: val_acc did not improve from 0.36679\n",
            "33500/33500 [==============================] - 7s 207us/sample - loss: 1.8385 - acc: 0.3731 - val_loss: 1.8809 - val_acc: 0.3526\n",
            "Epoch 11/100\n",
            "33440/33500 [============================>.] - ETA: 0s - loss: 1.8329 - acc: 0.3755\n",
            "Epoch 00011: val_acc improved from 0.36679 to 0.36891, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 204us/sample - loss: 1.8330 - acc: 0.3755 - val_loss: 1.8451 - val_acc: 0.3689\n",
            "Epoch 12/100\n",
            "33248/33500 [============================>.] - ETA: 0s - loss: 1.8195 - acc: 0.3805\n",
            "Epoch 00012: val_acc did not improve from 0.36891\n",
            "33500/33500 [==============================] - 7s 211us/sample - loss: 1.8195 - acc: 0.3805 - val_loss: 1.8623 - val_acc: 0.3675\n",
            "Epoch 13/100\n",
            "33312/33500 [============================>.] - ETA: 0s - loss: 1.8144 - acc: 0.3836\n",
            "Epoch 00013: val_acc improved from 0.36891 to 0.37594, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 210us/sample - loss: 1.8142 - acc: 0.3839 - val_loss: 1.8378 - val_acc: 0.3759\n",
            "Epoch 14/100\n",
            "33248/33500 [============================>.] - ETA: 0s - loss: 1.8102 - acc: 0.3824\n",
            "Epoch 00014: val_acc did not improve from 0.37594\n",
            "33500/33500 [==============================] - 7s 211us/sample - loss: 1.8097 - acc: 0.3823 - val_loss: 1.8847 - val_acc: 0.3608\n",
            "Epoch 15/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.8027 - acc: 0.3882\n",
            "Epoch 00015: val_acc did not improve from 0.37594\n",
            "33500/33500 [==============================] - 7s 211us/sample - loss: 1.8031 - acc: 0.3881 - val_loss: 1.8851 - val_acc: 0.3604\n",
            "Epoch 16/100\n",
            "33248/33500 [============================>.] - ETA: 0s - loss: 1.8038 - acc: 0.3862\n",
            "Epoch 00016: val_acc improved from 0.37594 to 0.38133, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 202us/sample - loss: 1.8041 - acc: 0.3861 - val_loss: 1.8279 - val_acc: 0.3813\n",
            "Epoch 17/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.7914 - acc: 0.3943\n",
            "Epoch 00017: val_acc did not improve from 0.38133\n",
            "33500/33500 [==============================] - 7s 224us/sample - loss: 1.7914 - acc: 0.3943 - val_loss: 1.8904 - val_acc: 0.3560\n",
            "Epoch 18/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.7943 - acc: 0.3929\n",
            "Epoch 00018: val_acc improved from 0.38133 to 0.38745, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 214us/sample - loss: 1.7942 - acc: 0.3931 - val_loss: 1.8380 - val_acc: 0.3875\n",
            "Epoch 19/100\n",
            "33312/33500 [============================>.] - ETA: 0s - loss: 1.7902 - acc: 0.3969\n",
            "Epoch 00019: val_acc did not improve from 0.38745\n",
            "33500/33500 [==============================] - 7s 214us/sample - loss: 1.7897 - acc: 0.3971 - val_loss: 1.8336 - val_acc: 0.3806\n",
            "Epoch 20/100\n",
            "33312/33500 [============================>.] - ETA: 0s - loss: 1.7817 - acc: 0.3974\n",
            "Epoch 00020: val_acc did not improve from 0.38745\n",
            "33500/33500 [==============================] - 7s 216us/sample - loss: 1.7824 - acc: 0.3973 - val_loss: 1.8485 - val_acc: 0.3747\n",
            "Epoch 21/100\n",
            "33440/33500 [============================>.] - ETA: 0s - loss: 1.7817 - acc: 0.3981\n",
            "Epoch 00021: val_acc did not improve from 0.38745\n",
            "33500/33500 [==============================] - 7s 211us/sample - loss: 1.7816 - acc: 0.3981 - val_loss: 1.8423 - val_acc: 0.3835\n",
            "Epoch 22/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7805 - acc: 0.3996\n",
            "Epoch 00022: val_acc did not improve from 0.38745\n",
            "33500/33500 [==============================] - 7s 214us/sample - loss: 1.7805 - acc: 0.3996 - val_loss: 1.9383 - val_acc: 0.3572\n",
            "Epoch 23/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7751 - acc: 0.4025\n",
            "Epoch 00023: val_acc did not improve from 0.38745\n",
            "33500/33500 [==============================] - 7s 206us/sample - loss: 1.7755 - acc: 0.4024 - val_loss: 1.8471 - val_acc: 0.3747\n",
            "Epoch 24/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.7730 - acc: 0.4040\n",
            "Epoch 00024: val_acc did not improve from 0.38745\n",
            "33500/33500 [==============================] - 7s 212us/sample - loss: 1.7731 - acc: 0.4041 - val_loss: 1.8360 - val_acc: 0.3852\n",
            "Epoch 25/100\n",
            "33376/33500 [============================>.] - ETA: 0s - loss: 1.7735 - acc: 0.4018\n",
            "Epoch 00025: val_acc improved from 0.38745 to 0.38800, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 215us/sample - loss: 1.7738 - acc: 0.4017 - val_loss: 1.8224 - val_acc: 0.3880\n",
            "Epoch 26/100\n",
            "33248/33500 [============================>.] - ETA: 0s - loss: 1.7687 - acc: 0.4037\n",
            "Epoch 00026: val_acc did not improve from 0.38800\n",
            "33500/33500 [==============================] - 7s 198us/sample - loss: 1.7687 - acc: 0.4039 - val_loss: 1.8394 - val_acc: 0.3816\n",
            "Epoch 27/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7633 - acc: 0.4087\n",
            "Epoch 00027: val_acc improved from 0.38800 to 0.38903, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 203us/sample - loss: 1.7631 - acc: 0.4088 - val_loss: 1.8307 - val_acc: 0.3890\n",
            "Epoch 28/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.7665 - acc: 0.4066\n",
            "Epoch 00028: val_acc did not improve from 0.38903\n",
            "33500/33500 [==============================] - 7s 200us/sample - loss: 1.7656 - acc: 0.4067 - val_loss: 1.8207 - val_acc: 0.3888\n",
            "Epoch 29/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7651 - acc: 0.4083\n",
            "Epoch 00029: val_acc did not improve from 0.38903\n",
            "33500/33500 [==============================] - 7s 208us/sample - loss: 1.7646 - acc: 0.4084 - val_loss: 1.8350 - val_acc: 0.3844\n",
            "Epoch 30/100\n",
            "33440/33500 [============================>.] - ETA: 0s - loss: 1.7561 - acc: 0.4104\n",
            "Epoch 00030: val_acc did not improve from 0.38903\n",
            "33500/33500 [==============================] - 7s 206us/sample - loss: 1.7562 - acc: 0.4102 - val_loss: 1.8422 - val_acc: 0.3773\n",
            "Epoch 31/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.7527 - acc: 0.4116\n",
            "Epoch 00031: val_acc did not improve from 0.38903\n",
            "33500/33500 [==============================] - 7s 207us/sample - loss: 1.7537 - acc: 0.4113 - val_loss: 1.8545 - val_acc: 0.3731\n",
            "Epoch 32/100\n",
            "33248/33500 [============================>.] - ETA: 0s - loss: 1.7559 - acc: 0.4107\n",
            "Epoch 00032: val_acc improved from 0.38903 to 0.39709, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 7s 208us/sample - loss: 1.7563 - acc: 0.4110 - val_loss: 1.8077 - val_acc: 0.3971\n",
            "Epoch 33/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7524 - acc: 0.4093\n",
            "Epoch 00033: val_acc did not improve from 0.39709\n",
            "33500/33500 [==============================] - 7s 222us/sample - loss: 1.7526 - acc: 0.4091 - val_loss: 1.8208 - val_acc: 0.3889\n",
            "Epoch 34/100\n",
            "33472/33500 [============================>.] - ETA: 0s - loss: 1.7484 - acc: 0.4109\n",
            "Epoch 00034: val_acc did not improve from 0.39709\n",
            "33500/33500 [==============================] - 7s 211us/sample - loss: 1.7486 - acc: 0.4109 - val_loss: 1.8213 - val_acc: 0.3879\n",
            "Epoch 35/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.7491 - acc: 0.4140\n",
            "Epoch 00035: val_acc did not improve from 0.39709\n",
            "33500/33500 [==============================] - 7s 209us/sample - loss: 1.7497 - acc: 0.4138 - val_loss: 1.8294 - val_acc: 0.3799\n",
            "Epoch 36/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7452 - acc: 0.4153\n",
            "Epoch 00036: val_acc did not improve from 0.39709\n",
            "33500/33500 [==============================] - 7s 199us/sample - loss: 1.7453 - acc: 0.4153 - val_loss: 1.8118 - val_acc: 0.3932\n",
            "Epoch 37/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7455 - acc: 0.4130\n",
            "Epoch 00037: val_acc did not improve from 0.39709\n",
            "33500/33500 [==============================] - 7s 197us/sample - loss: 1.7455 - acc: 0.4131 - val_loss: 1.8869 - val_acc: 0.3669\n",
            "Epoch 38/100\n",
            "33216/33500 [============================>.] - ETA: 0s - loss: 1.7503 - acc: 0.4120\n",
            "Epoch 00038: val_acc did not improve from 0.39709\n",
            "33500/33500 [==============================] - 7s 205us/sample - loss: 1.7508 - acc: 0.4119 - val_loss: 1.8509 - val_acc: 0.3794\n",
            "Epoch 39/100\n",
            "33440/33500 [============================>.] - ETA: 0s - loss: 1.7448 - acc: 0.4149\n",
            "Epoch 00039: val_acc did not improve from 0.39709\n",
            "33500/33500 [==============================] - 7s 206us/sample - loss: 1.7449 - acc: 0.4148 - val_loss: 1.8682 - val_acc: 0.3738\n",
            "Epoch 40/100\n",
            "33344/33500 [============================>.] - ETA: 0s - loss: 1.7484 - acc: 0.4143\n",
            "Epoch 00040: val_acc did not improve from 0.39709\n",
            "33500/33500 [==============================] - 8s 225us/sample - loss: 1.7485 - acc: 0.4141 - val_loss: 1.8213 - val_acc: 0.3889\n",
            "Epoch 41/100\n",
            "33248/33500 [============================>.] - ETA: 0s - loss: 1.7410 - acc: 0.4169\n",
            "Epoch 00041: val_acc did not improve from 0.39709\n",
            "33500/33500 [==============================] - 7s 217us/sample - loss: 1.7417 - acc: 0.4166 - val_loss: 1.8857 - val_acc: 0.3650\n",
            "Epoch 42/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7448 - acc: 0.4144\n",
            "Epoch 00042: val_acc did not improve from 0.39709\n",
            "33500/33500 [==============================] - 8s 226us/sample - loss: 1.7455 - acc: 0.4140 - val_loss: 1.8570 - val_acc: 0.3788\n",
            "Epoch 00042: early stopping\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: 1.7961 - acc: 0.3942\n",
            "\n",
            "Test accuracy: 39.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Q2l4KaYhvb",
        "colab_type": "text"
      },
      "source": [
        "Repeat past computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_svvL3hhS_yv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb81af43-0418-49e6-d5e0-7f34552df496"
      },
      "source": [
        "# Changed batch_size v2\n",
        "batch_size = 128\n",
        "\n",
        "# Regularizer\n",
        "kernel_regularizer = l2(0.0001)\n",
        "\n",
        "# this is 3-layer MLP with ReLU. With L2 Regularizer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units, kernel_regularizer=kernel_regularizer))\n",
        "model.add(Activation('relu'))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of adam optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# simple early stopping\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n",
        "\n",
        "\n",
        "model.fit(x_train_gray, y_train, batch_size=batch_size, nb_epoch=100,\n",
        "      shuffle=True, verbose=1, callbacks=[es, mc], validation_split=0.33)\n",
        "\n",
        "\n",
        "saved_model = load_model('best_model.h5')\n",
        "\n",
        "loss, acc = saved_model.evaluate(x_test_gray, y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_49 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 396,554\n",
            "Trainable params: 396,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "Train on 33500 samples, validate on 16500 samples\n",
            "Epoch 1/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 2.1332 - acc: 0.2435\n",
            "Epoch 00001: val_acc improved from -inf to 0.29600, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 6s 165us/sample - loss: 2.1324 - acc: 0.2438 - val_loss: 2.0148 - val_acc: 0.2960\n",
            "Epoch 2/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.9646 - acc: 0.3119\n",
            "Epoch 00002: val_acc improved from 0.29600 to 0.32055, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 99us/sample - loss: 1.9643 - acc: 0.3120 - val_loss: 1.9320 - val_acc: 0.3205\n",
            "Epoch 3/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.8852 - acc: 0.3428\n",
            "Epoch 00003: val_acc improved from 0.32055 to 0.34200, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.8853 - acc: 0.3427 - val_loss: 1.8718 - val_acc: 0.3420\n",
            "Epoch 4/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.8318 - acc: 0.3599\n",
            "Epoch 00004: val_acc did not improve from 0.34200\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.8319 - acc: 0.3601 - val_loss: 1.8780 - val_acc: 0.3405\n",
            "Epoch 5/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.7914 - acc: 0.3779\n",
            "Epoch 00005: val_acc improved from 0.34200 to 0.37182, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.7915 - acc: 0.3780 - val_loss: 1.8024 - val_acc: 0.3718\n",
            "Epoch 6/100\n",
            "32768/33500 [============================>.] - ETA: 0s - loss: 1.7589 - acc: 0.3904\n",
            "Epoch 00006: val_acc improved from 0.37182 to 0.37224, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.7589 - acc: 0.3907 - val_loss: 1.8118 - val_acc: 0.3722\n",
            "Epoch 7/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.7271 - acc: 0.4006\n",
            "Epoch 00007: val_acc improved from 0.37224 to 0.39109, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.7269 - acc: 0.4008 - val_loss: 1.7524 - val_acc: 0.3911\n",
            "Epoch 8/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.7020 - acc: 0.4039\n",
            "Epoch 00008: val_acc did not improve from 0.39109\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.7012 - acc: 0.4043 - val_loss: 1.7718 - val_acc: 0.3865\n",
            "Epoch 9/100\n",
            "32768/33500 [============================>.] - ETA: 0s - loss: 1.6774 - acc: 0.4191\n",
            "Epoch 00009: val_acc improved from 0.39109 to 0.39315, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.6776 - acc: 0.4186 - val_loss: 1.7527 - val_acc: 0.3932\n",
            "Epoch 10/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.6494 - acc: 0.4287\n",
            "Epoch 00010: val_acc did not improve from 0.39315\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.6496 - acc: 0.4287 - val_loss: 1.7652 - val_acc: 0.3912\n",
            "Epoch 11/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.6342 - acc: 0.4328\n",
            "Epoch 00011: val_acc improved from 0.39315 to 0.39970, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 100us/sample - loss: 1.6340 - acc: 0.4330 - val_loss: 1.7436 - val_acc: 0.3997\n",
            "Epoch 12/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.6057 - acc: 0.4446\n",
            "Epoch 00012: val_acc improved from 0.39970 to 0.41273, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 100us/sample - loss: 1.6055 - acc: 0.4445 - val_loss: 1.7044 - val_acc: 0.4127\n",
            "Epoch 13/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.5921 - acc: 0.4505\n",
            "Epoch 00013: val_acc did not improve from 0.41273\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.5923 - acc: 0.4507 - val_loss: 1.7455 - val_acc: 0.3990\n",
            "Epoch 14/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.5704 - acc: 0.4578\n",
            "Epoch 00014: val_acc did not improve from 0.41273\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.5705 - acc: 0.4577 - val_loss: 1.7248 - val_acc: 0.4022\n",
            "Epoch 15/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.5563 - acc: 0.4621\n",
            "Epoch 00015: val_acc did not improve from 0.41273\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 1.5552 - acc: 0.4624 - val_loss: 1.7298 - val_acc: 0.4037\n",
            "Epoch 16/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.5306 - acc: 0.4755\n",
            "Epoch 00016: val_acc improved from 0.41273 to 0.41327, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.5297 - acc: 0.4760 - val_loss: 1.7307 - val_acc: 0.4133\n",
            "Epoch 17/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.5137 - acc: 0.4845\n",
            "Epoch 00017: val_acc improved from 0.41327 to 0.42133, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.5143 - acc: 0.4840 - val_loss: 1.7117 - val_acc: 0.4213\n",
            "Epoch 18/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.4909 - acc: 0.4890\n",
            "Epoch 00018: val_acc did not improve from 0.42133\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.4909 - acc: 0.4888 - val_loss: 1.7178 - val_acc: 0.4140\n",
            "Epoch 19/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.4796 - acc: 0.4948\n",
            "Epoch 00019: val_acc improved from 0.42133 to 0.42188, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 94us/sample - loss: 1.4792 - acc: 0.4948 - val_loss: 1.7146 - val_acc: 0.4219\n",
            "Epoch 20/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.4623 - acc: 0.5032\n",
            "Epoch 00020: val_acc did not improve from 0.42188\n",
            "33500/33500 [==============================] - 3s 89us/sample - loss: 1.4616 - acc: 0.5035 - val_loss: 1.7020 - val_acc: 0.4203\n",
            "Epoch 21/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.4406 - acc: 0.5110\n",
            "Epoch 00021: val_acc improved from 0.42188 to 0.42509, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.4409 - acc: 0.5111 - val_loss: 1.7106 - val_acc: 0.4251\n",
            "Epoch 22/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.4285 - acc: 0.5142\n",
            "Epoch 00022: val_acc did not improve from 0.42509\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.4286 - acc: 0.5147 - val_loss: 1.7421 - val_acc: 0.4199\n",
            "Epoch 23/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.4102 - acc: 0.5206\n",
            "Epoch 00023: val_acc improved from 0.42509 to 0.42770, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 91us/sample - loss: 1.4104 - acc: 0.5205 - val_loss: 1.7312 - val_acc: 0.4277\n",
            "Epoch 24/100\n",
            "33280/33500 [============================>.] - ETA: 0s - loss: 1.3895 - acc: 0.5321\n",
            "Epoch 00024: val_acc did not improve from 0.42770\n",
            "33500/33500 [==============================] - 3s 90us/sample - loss: 1.3898 - acc: 0.5321 - val_loss: 1.7337 - val_acc: 0.4223\n",
            "Epoch 25/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.3806 - acc: 0.5370\n",
            "Epoch 00025: val_acc improved from 0.42770 to 0.43079, saving model to best_model.h5\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.3809 - acc: 0.5369 - val_loss: 1.7408 - val_acc: 0.4308\n",
            "Epoch 26/100\n",
            "33408/33500 [============================>.] - ETA: 0s - loss: 1.3448 - acc: 0.5507\n",
            "Epoch 00026: val_acc did not improve from 0.43079\n",
            "33500/33500 [==============================] - 3s 97us/sample - loss: 1.3446 - acc: 0.5508 - val_loss: 1.7585 - val_acc: 0.4302\n",
            "Epoch 27/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.3322 - acc: 0.5567\n",
            "Epoch 00027: val_acc did not improve from 0.43079\n",
            "33500/33500 [==============================] - 3s 103us/sample - loss: 1.3318 - acc: 0.5570 - val_loss: 1.7884 - val_acc: 0.4227\n",
            "Epoch 28/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.3157 - acc: 0.5610\n",
            "Epoch 00028: val_acc did not improve from 0.43079\n",
            "33500/33500 [==============================] - 3s 104us/sample - loss: 1.3169 - acc: 0.5606 - val_loss: 1.7935 - val_acc: 0.4255\n",
            "Epoch 29/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.2964 - acc: 0.5724\n",
            "Epoch 00029: val_acc did not improve from 0.43079\n",
            "33500/33500 [==============================] - 3s 95us/sample - loss: 1.2965 - acc: 0.5726 - val_loss: 1.8065 - val_acc: 0.4295\n",
            "Epoch 30/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.2749 - acc: 0.5768\n",
            "Epoch 00030: val_acc did not improve from 0.43079\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.2754 - acc: 0.5764 - val_loss: 1.8094 - val_acc: 0.4212\n",
            "Epoch 31/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.2660 - acc: 0.5826\n",
            "Epoch 00031: val_acc did not improve from 0.43079\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.2668 - acc: 0.5825 - val_loss: 1.8592 - val_acc: 0.4242\n",
            "Epoch 32/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.2402 - acc: 0.5931\n",
            "Epoch 00032: val_acc did not improve from 0.43079\n",
            "33500/33500 [==============================] - 3s 96us/sample - loss: 1.2411 - acc: 0.5929 - val_loss: 1.8523 - val_acc: 0.4212\n",
            "Epoch 33/100\n",
            "33024/33500 [============================>.] - ETA: 0s - loss: 1.2261 - acc: 0.5982\n",
            "Epoch 00033: val_acc did not improve from 0.43079\n",
            "33500/33500 [==============================] - 3s 93us/sample - loss: 1.2281 - acc: 0.5978 - val_loss: 1.8913 - val_acc: 0.4118\n",
            "Epoch 34/100\n",
            "32896/33500 [============================>.] - ETA: 0s - loss: 1.2120 - acc: 0.6041\n",
            "Epoch 00034: val_acc did not improve from 0.43079\n",
            "33500/33500 [==============================] - 3s 98us/sample - loss: 1.2126 - acc: 0.6036 - val_loss: 1.8905 - val_acc: 0.4210\n",
            "Epoch 35/100\n",
            "33152/33500 [============================>.] - ETA: 0s - loss: 1.1992 - acc: 0.6111\n",
            "Epoch 00035: val_acc did not improve from 0.43079\n",
            "33500/33500 [==============================] - 3s 102us/sample - loss: 1.2004 - acc: 0.6104 - val_loss: 1.8812 - val_acc: 0.4263\n",
            "Epoch 00035: early stopping\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.7410 - acc: 0.4261\n",
            "\n",
            "Test accuracy: 42.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AywVXFnbYhBQ",
        "colab_type": "text"
      },
      "source": [
        "Still is the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lhRzifgZi44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}